---
title: "[[Promotion]]"
type: Permanent
status: ing
Creation Date: 2025-09-30 16:09
tags:
---
## 自我介绍
大家好，我是来自平台团队的小刘博，大家也可以叫我阿岛，我毕业于东华大学，本科和研究生均就读于东华大学，2023年我完成了我的硕士答辩，拿到了联想的offer，以unity开发工程师的身份，加入联想研究院。我在Unity开发领域拥有2年的实际工作经验，目前主要使用Unity引擎进行DW客户端开发。

关于我个人呢，我的爱好比较广泛，我喜欢徒步、攀岩、diy。接下来介绍一下我在DW团队中的角色与定位

## 角色与定位
我在团队中承担**Unity 开发**的角色，目前我的工作围绕「DW Remote」来开展，这张图展示的是Remote的整体架构。它可以分为四个层次：业务层、应用层、支撑层和服务层。

在业务层，我负责核心业务的开发，实现基本的使用流程。
在应用层，我设计了应用的启动流程，保证稳定性。设计了一个错误上报机制，记录运行时的状态异常。
在能力支撑层，我创建了输入模块，简化了客户端设备的适配流程
采用了响应式编程的思想，构建了机器人数据管理模块和事件分发的机制，让客户端对‘用户操作’‘机器人状态变化’的响应更及时。
基于**URP 渲染管线**和**GPGPU 技术**，我从‘渲染’和‘计算’两个维度对点云模块进行了优化，降低了渲染延迟

最后，remote是多团队协作的结果，所以我需要和算法团队、硬件团队、后端深度联调，推动跨团队问题闭环
接下来，分享一下我主要的工作成果。

## 3DGS渲染部分
第一个成果是我首次将3DGS渲染技术，集成到了DW平台中，让我们的平台具有了虚实融合的关键能力

在项目初期，我首先要解决的是3DGS的工程化落地问题。3DGS SDK的虚实融合方案采用多相机进行渲染，逻辑高度耦合且复杂，导致渲染开销大，性能差。而且复杂的逻辑分散在多个渲染循环中，后续的迭代和问题排查会比较难。
>

基于对通用渲染管线的深度理解，我实现了一个‘单相机注入式’的虚实融合方案。可以让3DGS无缝地融入到现有的渲染流程中，我利用渲染管线自动的对数据进行了处理，整个过程分为两个步骤：
1. 第一步，在深度排序之前，将3DGS场景的深度信息注入管线。利用管线，自动处理物体的遮挡关系。
2. 第二步，在绘制其他物体之前，我将3DGS的纹理注入管线，作为画面绘制的背景。这样一来，虚拟物体就会在3DGS的基础上进行绘制，实现虚实融合的效果
如果把虚实融合的过程比喻成印刷的话，之前的方案需要根据不同颜色分为不同的图层多次印刷，实现最终效果，而新方案配置好了各个区域的颜色，只有一个图层，只需要印刷一次

而且新方案只有一个渲染循环，剔除、场景设置等耗时操作只执行一次，之前的方案至少执行三次，**GPU开销比之前降低了5%**。在1080P分辨率下可以**节省32MB的显存**。而且这个方案可以轻松地与各种后处理效果结合，拓展性好

我遇到的第二个挑战来自**交互体验**：像这幅图展示的一样，用户在studio中编辑场景时，会出现严重的画面拖尾现象，破坏沉浸感。

通过深入分析，我定位到根源在于Studio的操作帧与3DGS的渲染帧是异步的。为此，我设计了一个‘帧锚定与双缓冲’的协同机制：

1. Studio的操作指令发出后，必须等待3DGS渲染完成的信号，才更新下一帧的位置，从而在逻辑上对齐了两者的时间线。
2. 同时，我引入了双缓冲机制，这边有两个缓冲区，轮流渲染3dgs的纹理，确保用户看到的永远是一幅已经完整渲染的图像，彻底消除了拖尾现象。实现了平滑的编辑体验。

实际的渲染效果如图所示，虚实遮挡的效果精确，在保证高质量渲染的前提下，GPU耗时减半。这套完整的3DGS集成方案，是一个高度模块化、可复用的解决方案。它即插即用，对原有场景零侵入，降低了公司内其他项目采纳3DGS技术的门槛。从Studio到现在的Deploy，这套方案一直在稳定工作。

## 点云渲染部分
接下来，讲一下我为移动端构建的一套高性能、低延迟的点云渲染管线。提高了我们产品在移动端进行环境感知的能力。

Remote首先是一个**高响应的控制应用**，所以CPU资源必须优先保障核心模块。渲染模块如果占用了过多的CPU资源，就会导致应用卡顿，这样就本末倒置了。最初的方案，就有这样的问题。开启点云渲染之后，会占用50%的内存，应用会变得卡顿，平均帧率都不到30帧率，这个时候去操作机器人，机器人的响应都不及时了。我们付出这么大的代价得到的是什么呢？像这里展示的一样，我们得到的是渲染延迟超过2s的一个点云展示，我们看到的是2s之前的点云情况，用户在远程操作的时候，也不得不走走停停，等待点云的更新。



我们来看一下之前的方案，它大量的计算密集型任务都堆在CPU上，包括数据解析、坐标转换、分帧处理等等，在这个结构下，渲染100个点的点云就需要调用100次绘制命令，效率低下，而且也不稳定，数据量一大，cpu就会被卡死。我将整个渲染流程重新设计，把所有繁重的计算任务，从CPU‘卸载’，迁移到擅长大规模并行计算的GPU上。

在这个新架构下，CPU的职责被简化到极致：仅负责接收网络数据，然后将数据传递给Gpu
GPU会通过成百上千的核心同时对数据进行解析，将点云的位置、颜色信息瞬间解析到显存中。
接下来是一个基于索引的数据管理机制。首次加载走‘全量更新’通道，后续的数据变化则通过‘增量更新’通道，只更新变化的点，极大地降低了数据传输和处理的开销。
为了近一步节省资源，我在GPU侧实现了视锥剔除。从图中可以看到，通过剔除，我们只渲染用户视野内的点，避免了其他不必要的绘制，而且渲染效果也没有受到影响。

这个方案带来的优化是巨大的：
- **首先，它的内存占用就减少到了5%，给cpu提供了额外的计算冗余** 
- 其次，这是一个极致的CPU友好型方案，开启渲染后，应用的帧率还可以到60帧率。保证了remote的流畅、稳定、高响应。
- 最终，在我们主流的出货设备上，这套方案可以实现500ms内的渲染延迟。这意味着用户能够获得接近实时的环境感知体验，构成了我们产品的一个竞争力。

## 自适应框架
第三个成果，是面向机器人构建了一套自适应的控制框架。

remote需要支持不同的客户端设备、不同型号的机器人；还需要支持不同的负载。这里展示的是，remote核心逻辑对设备类型、机器人型号、负责类型的依赖，几乎所有的逻辑模块都要依赖这三个数据。这些纵横交错的逻辑链，导致研发效率低，适配周期长。

在这边依次解决这三个数据的依赖问题，首先是设备类型的问题，我们的控制逻辑与具体硬件深度耦合，每接入一个新设备，就要重写一套控制逻辑。所以我设计了一个输入适配层，将设备的输入信号归一化，向上抽象为统一的‘Action’，通过这个抽象层解耦了硬件设备与控制逻辑，使得所有设备共享一套控制代码。

第2个问题，是机器人型号的问题。不同型号的机器人，其状态和能力千差万别，导致代码中充斥着大量的判断逻辑，整个系统变得脆弱且难以维护。在新框架中，我不再关注‘机器人的型号’，而是关注‘机器人的能力’。通过策略模式来对不同型号的机器人进行能力的组装。这里对机器人的数据更新逻辑进行了隔离，机器人具备的能力也进行了定义。客户端会根据机器人的能力，动态地展示对应的UI。这样一来，过去繁琐的UI适配工作，变成了一个自动化的、可配置的流程，提升了我们适配机器人的效率。

第3个问题，是机器人负载的问题。不同负载的功能调用逻辑不同，阻碍了负载的模块化扩展。在这里，我定义了统一的Payload接口，可以通过SDK或外部配置文件来加载负载。使用命令模式，将负载的所有操作封装成独立的、包含自验证和执行逻辑的‘指令’。同时，负载的状态变化会通过统一的事件系统分发出去，驱动UI更新。

框架的整体结构就在这里，这部分是输入模块的展开，这部分是负载部分的展开。这个框架将我们的开发模式从‘面向具体型号’，提升到了‘面向抽象能力’的更高维度。之前我们进行端到端适配的时候，至少要修改十几个文件，现在我们只需要在策略模式的脚本中进行配置就好了，就好比在安卓系统中去开发应用，我们不需要关注硬件的具体型号，我们只需要调用安卓系统中对应的功能接口就好了。
这个框架至少可以减少一半的机器人适配时间，让研发把精力放在新功能开发上，它提升了开发效率和remote的长期可扩展性。

## Think & Key Learning
主要的工作内容就是这些，在这一页我想讲一下我对remote和机器人的一些思考。
第一点是智能决策，我之前在户外测试狗的时候，因为户外温度高，导致机器狗站立了一会儿，关节温度就异常升高，狗直接趴下了。我们的狗检测到这种异常的温度升高之后，能不能去采取一些措施呢？比如说先趴下让温度不至于上升太快，或者更智能一点，找一个凉快的地方呆着。
第二点是集群协同工作，我看到云深处有这样的尝试，利用多个机器狗在未知环境下进行全自主的协同搜索。
这两幅图是我用AI生成的，我觉得我们在具身智能领域还有很大的想想空间！

## Future
最后，我展望一下未来。
在业务方面，我希望在代码架构这一方面有所提高，架构是一个app的骨骼，数据是血液、逻辑是肌肉，一个好的架构也会让我比较有成就感
在技术方面，我想多学习渲染方面的知识，尤其是GPGPU相关的技术与应用。

## question
**你提到了“多相机虚拟-现实融合方案”的复杂性和性能问题。能更详细地解释一下这个方案的具体工作原理吗？它为什么会导致“渲染开销大”和“性能差”？**

多相机的虚实融合方案通过多个相机分别渲染虚拟物体和场景地图的深度、纹理信息，通过比较场景地图和虚拟物体的深度，计算纹理的显示优先级
CPU开销巨大：重复的“准备工作”
- **剔除 (Culling)、设置渲染状态、发送指令 (Draw Call)**
GPU开销增加：带宽消耗和上下文切换
- **显存带宽压力**: GPU在整个过程需要频繁地在显存中读写数据。它先把场景数据写入RT_B和RT_C，然后在最后合成阶段又要把这些RT的数据**读出来**进行计算。这一来一回地搬运大量像素数据（尤其是在高分辨率下）会消耗巨大的**显存带宽**

**GPU开销降低5%对整体的用户体验影响有多大？这5%主要节省在了哪个环节（例如，Fillrate、Vertex Processing、Bandwidth）？**
GPU的渲染时间降低了5%，10ms左右，GPU的带宽消耗和上下文环境的切换耗时减少

**PPT中的Before/After图展示的是什么数据？能解释一下这些表格数据代表的含义和优化效果吗？**

gpu对显存中的纹理数据进行读写的时间小了，这个小的幅度更大，导致我们的时间占总体的比值反而升高了

**点云渲染行业内的解决方案**

我们不能照搬行业内那些‘重型’的、渲染百万级静态点云的方案。因为他们的核心是‘调度海量数据’。而DW Remote的**挑战是‘在有限的CPU资源下，保证实时数据的流畅响应’**。

如何实现这种CPU到GPU的任务迁移的？（例如，Compute Shader是肯定要提到的，可以详细展开）

**“增量更新”的具体实现原理是什么？如何判断哪些点发生了变化？**


**你提到了“长R&D周期”和“低R&D效率”。具体而言，在你的旧方案中，开发一个新功能或适配一个新的客户端设备/机器人/负载，通常需要多长时间？新框架能将这个时间缩短到多少**
2-4天缩短到1-2天

**“减少一半的机器人适配时间”——这是一个非常具体的量化指标。你是如何衡量这个“适配时间”的？是否有数据支撑这个结论？**

自己的适配经历，和代码量的对比，其实这是一个保守数据，代码的修改效率是提高了的