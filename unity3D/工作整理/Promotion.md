---
title: "[[Promotion]]"
type: Permanent
status: ing
Creation Date: 2025-09-30 16:09
tags:
---
## 自我介绍
大家好，我是来自刘博团队的小刘博，我毕业于东华大学，本科和研究生均就读于东华大学，2021年我完成了我的学士答辩，2023年结束了我的硕士生涯，同年我拿到了咱们联想的offer，以unity开发工程师的身份，正式加入联想研究院的大家庭。我在Unity开发领域拥有一年的实际工作经验，目前主要使用Unity引擎进行DW客户端开发。

关于我个人呢，我的爱好比较广泛，我喜欢徒步，本科期间组织过多次徒步活动，和小伙伴一起游山玩水。我喜欢攀岩，研究生期间，代表学校参加了上海市的攀岩锦标赛。我喜欢DIY一些东西，入职联想之后也完成了自己第一个3D打印机。

接下来，我简单介绍我在DW前端团队中的角色与定位

## 角色与定位
在DW的架构中，用户可以通过DW Tools 定制化的开发一些内容，并且使用 Editor进行场景构建与编辑，最终发布到DW View中进行呈现。作为一名Unity开发工程师，我主要负责DW View的混合渲染组件和多终端部署。

关于混合渲染，DW的核心理念是一切资产，所以，DW 运营的一个重要职责就是把提升数字孪生混合渲染效果的技术封装成资产组件，方便方案团队快速调用。我的职责之一就是构建这些资产组件。在这部分工作中我首次将3DGS渲染引擎集成到DW平台中，使得用户可以在nerf环境下进行三维场景的编辑；同时我推动了信息展示、位置跳转、自动导览等组件的研发与落地，提升了DW的展览展示效果。

关于多终端部署，多终端部署的愿景就是DW构建的场景资产包可以在不同的终端上展现，并实现无缝切换。在这部分工作中我对ViewerAR的跨平台逻辑进行了整合，并基于此完成了ViewerAR WebGL平台的适配，助力了VIewerAR微信小程序的落地，同时我还参与设计了 DW studio 运营端多平台发布的程序框架，对studio 移动端的输入事件进行了适配

接下来，对我的主要的工作成果，展开分享一下

## 3DGS渲染部分
我今天汇报的第一个核心贡献，是我首次将3DGS渲染技术，成功工程化并集成到了我们的DW平台中，让我们的平台具有了虚实融合的关键能力

在项目初期，我首先要解决的是3DGS的工程化落地问题。3DGS SDK的多相机虚实融合方案，虽然能实现效果，但存在两个问题：
>多相机的虚实融合方案通过多个相机分别渲染虚拟物体和场景地图的深度、纹理信息，通过比较场景地图和虚拟物体的深度，计算纹理的显示优先级

1. **架构臃肿：** 多相机渲染的流程高度耦合且复杂，导致渲染开销巨大，性能表现远不达标。
2. **可维护性差：** 复杂的逻辑分散在多个相机和渲染循环中，后续的迭代和问题排查会非常困难。

基于对通用渲染管线的深度理解，我构思并实现了一个的‘单相机注入式’虚实融合方案。

这个方案的核心思想，是让3DGS无缝地融入到我们现有的渲染流程中，而不是在外部进行复杂的拼凑。我的具体做法是：
1. **深度注入：** 在渲染管线进行深度排序（Z-Test）之前，我将3DGS场景的深度信息‘喂’给主相机。这样一来，我们就能**巧妙地利用引擎原生的深度比较能力**，自动处理所有物体的遮挡关系。
2. **纹理注入：** 在绘制不透明物体之前，我将3DGS渲染好的纹理作为背景注入。

**这个渲染流程的优化，带来了立竿见影的成果：** 整个流程只有一个渲染循环，CPU侧的剔除和场景设置等耗时操作只执行一次，相比之前方案的至少三次，**CPU开销降低了60%**，Draw Call也显著减少。”

在资源占用上，由于所有操作都在主相机的帧缓冲区内完成，我们不再需要额外的Render Texture，在1080P分辨率下就**节省了32MB的显存**。这对于未来在更多设备上的适配至关重要。更重要的是，这个方案的扩展性极佳，可以轻松地与各种后处理效果结合，为后续的视觉效果提升上，打下了基础。

解决了性能瓶颈后，我遇到的第二个核心挑战来自**交互体验**：在编辑场景时，出现了严重的画面拖尾现象，这完全破坏了我们所追求的沉浸感，是一个必须解决的用户体验问题。

通过深入分析，我定位到根源在于Studio的操作帧与3DGS的渲染帧是异步的。为此，我设计并实施了一套‘帧同步与双缓冲’的协同机制：

1. **握手同步：** 我建立了一套通信协议，确保Studio的操作指令发出后，必须等待3DGS渲染完成的信号，才更新下一帧的位置，从而在逻辑上对齐了两者的‘时间线’。
2. **双缓冲保护：** 同时，我引入了双缓冲机制，确保用户看到的永远是一幅已经完整渲染的图像，彻底消除了画面撕裂和拖尾。
通过这个系统性的解决方案，我们最终实现了平滑、沉浸的编辑体验。

最终，我交付的这套完整的3DGS集成方案，在保证高质量渲染的前提下，**实现了GPU耗时减半的卓越成果**。从Studio到最终的Deploy产品，这套方案一直在稳定工作，几乎无需修改，证明了其架构的稳健性。

但我想强调，这个项目的价值远不止于性能数字。我交付的，是一个真正**符合现代渲染管线思想的、高度模块化、可复用的解决方案**。它‘一键集成，即插即用，对原有场景零侵入’的特性，意味着它极大地降低了公司内其他项目采纳3DGS技术的门槛和成本。

## 点云渲染部分
接下来，我将向各位汇报我在DW Remote项目中，为移动端构建的一套高性能、低延迟的实时点云渲染管线。这项工作，直接解决了我们产品在移动端进行实时环境感知的一个技术瓶颈。

在移动端渲染实时变化的云端点云，我们面临的挑战是独特的。它不仅仅是硬件的限制，更是资源分配的问题。

我深入研究后发现，我们不能照搬行业内那些‘重型’的、渲染百万级静态点云的方案。因为他们的核心是‘调度海量数据’。而我们DW Remote的**核心挑战是‘在极度有限的CPU资源下，保证实时数据的流畅响应’**。”

“为什么这么说？因为DW Remote首先是一个**高响应的控制应用**，它的CPU资源是‘珍贵’的，必须优先保障网络通信、机器人状态解析和用户UI操作。渲染模块如果过度抢占CPU，导致主应用卡顿，那就是本末倒置。**因此，任何一个‘CPU不友好’的方案，对于我们的产品来说，都是不可接受的。**”

我们最初的方案，就陷入了这样的困境。它将大量的解析、转换、分帧逻辑都堆砌在CPU上，这不仅效率低下，而且也不稳定，可能会因为数据波动而导致整个应用失去响应。

我的设计想法很明确：**‘彻底解放CPU，把专业的事交给专家’**。我将整个渲染流程重新设计，把所有繁重的计算任务，从CPU‘卸载’，完全迁移到真正擅长大规模并行计算的GPU上。

在这个新架构下，CPU的职责被简化到极致：仅负责接收网络数据，然后像一个‘邮差’，**将原始二进制数据包原封不动地直接传递给GPU**。一旦数据进入GPU，一套高效的自动化流水线便开始工作：

1. **第一步：GPU原生解析。** 我利用Compute Shader，让成百上千的GPU核心同时解码数据，将点云的位置、颜色信息瞬间解析到显存中。
2. **第二步：智能增量更新。** 我设计了一套基于索引的数据管理机制。首次加载走‘全量更新’通道，后续的数据变化则通过‘增量更新’通道，精准地修改变化的点，极大地降低了数据传输和处理的开销。
3. **第三步：极致的渲染优化。**
    - **动态视锥剔除：** 为了将性能压榨到极致，我在GPU侧实现了视锥剔除。我们只渲染用户视野内的点，剔除所有不必要的绘制。
    - **GPU驱动的间接渲染：** 最关键的一步，我采用了间接渲染（Indirect Drawing）。这意味着连‘需要渲染多少个点’这个决策，都不再需要CPU来计算和下发指令。GPU在剔除后，自己就知道要画多少，自己驱动自己完成渲染。**CPU与GPU之间，从过去繁琐的‘指令式汇报’，变成了现在‘信任式授权’，通信开销降到了最低。**”
        
这个方案带来的优化是巨大的：
- **首先，它是一个极致的‘CPU友好型’方案。** 我们将CPU从繁重的计算中彻底解放，确保了DW Remote作为控制应用的核心功能，永远流畅、稳定、高响应。
- **其次，它的效率是惊人的。** 通过GPU并行处理，**点云数据的解析时间缩短为原来的1/10**，实现了从云端数据流到用户眼前的“零延迟”感知。
- **最终，它带来了一个好的产品体验。** 在我们主流的出货设备上，这套方案可以轻松实现**10万点云60FPS的满帧实时渲染**。这意味着用户能够获得丝般的实时环境感知体验，这构成了我们产品的一个竞争力。

## 自适应框架

我的第三个核心贡献，是**为我们整个异构机器人产品线，设计并构建了一套统一的、自适应的控制框架**。这项工作，从根本上解决了过去客户端因为‘碎片化’而导致的用户体验不一致、开发效率低下、功能扩展困难的三大问题。

我们面临的第一个困境，是**控制生态的碎片化**。我们的客户端需要支持G20专业遥控器和普通安卓手机，但由于缺乏顶层设计，导致控制逻辑与具体硬件深度耦合，形成了多个‘控制孤岛’。这不仅让用户在切换设备时感到体验割裂，更让我们的开发陷入了‘每接入一个新设备，就重写一套控制逻辑’的低效循环中。

所以我将所有不同设备的、物理形态各异的输入信号，向上抽象并归一化为统一的‘机器人Action’——无论你是推摇杆还是搓屏幕，其本质都是向机器人下达‘移动’或‘旋转’的意图。通过这个抽象层，我彻底解耦了硬件设备与控制逻辑，**所有设备现在共享同一套核心控制代码**。

不仅如此，我还考虑到了不同设备的用户体验。我为G20的物理摇杆设计了线性响应曲线，保持了其精准的物理反馈；同时，为手机的虚拟摇杆设计了‘缓入陡出’的非线性曲线，巧妙地解决了手机微操困难、行程短的问题。**这确保了我们在统一逻辑的基础上，还能提供符合各自设备特性的最优操控手感。**”

第二个困境，是**机器人功能的碎片化**。不同型号的机器人，其状态和能力千差万别。旧架构缺乏统一的管理机制，导致代码中充斥着大量的if-else判断，逻辑随着机器人型号的增多而指数级膨胀，**整个系统变得脆弱且难以维护**。

针对这个问题，我引入了‘能力驱动’的设计思想，并采用策略模式进行实现。我不再关注‘这是什么型号的机器人’，而是关注‘这个机器人具备哪些能力’。
- **数据更新策略化：** 我为不同机器人的数据更新逻辑（如关节数据、传感器数据）创建了独立的策略模块，实现了功能的‘高内聚、低耦合’。    
- **UI由能力驱动：** 更重要的是，我建立了一套由机器人‘能力集合’驱动的UI自适应机制。客户端会根据机器人上报的‘能力清单’，动态地、自动地展示对应的功能UI。比如，机器人说‘我会上楼梯’，UI上就自动出现爬楼模式的按钮。
这个优化的效果是很明显的，它将过去繁琐、易错的UI适配工作，变成了一个自动化的、可配置的流程，提升了我们接入新型号机器人的效率和稳定性。

第三个困境，是**机器人负载（Payload）管理的混乱**。云台、相机、喊话器等不同负载的功能调用逻辑各不相同，缺乏统一接口，导致负载无法做到‘即插即用’，阻碍了机器人功能的模块化扩展。”

为此，我设计了一套**标准化的、可动态配置的负载管理框架**。
- **统一接口与动态加载：** 我定义了统一的Payload接口，并支持通过SDK或外部配置文件两种方式动态加载负载模块。
- **指令化与事件驱动：** 我使用命令模式，将对负载的所有操作封装成独立的、包含自验证和执行逻辑的‘指令’。同时，负载的状态变化会通过统一的事件系统分发出去，自动驱动UI更新。

通过这套机制，我们实现了负载的**‘热插拔’和‘即插即用’**，为未来机器人功能的无限扩展构建了坚实的基础。”

总而言之，我交付的不仅仅是一个功能模块，而是一整套**面向未来的、可扩展的异构机器人控制框架**。它将我们的开发模式从‘面向具体设备和型号’，提升到了‘面向抽象能力和标准接口’的更高维度。它不仅为用户带来了多端一致的操控体验，更重要的是，它提升了我们团队的开发效率和整个机器人平台的长期可扩展性。

目前dwremote支持专业的遥控器G20，以及普通的安卓手机，由于**缺乏统一的输入与控制抽象**，控制代码与具体的硬件设备**深度耦合**。每当需要接入一种新的设备，开发人员就重写大量的适配代码，这就形成了一个个封闭的“控制孤岛”，每种设备都有自己单独的控制逻辑。这种碎片化的生态系统，最终导致了跨设备间操控体验的严重割裂，难以形成统一、流畅的用户体验。