---
title: "[[Promotion]]"
type: Permanent
status: ing
Creation Date: 2025-09-30 16:09
tags:
---
## 自我介绍
大家好，我是来自刘博团队的小刘博，大家也可以叫我阿岛，我毕业于东华大学，本科和研究生均就读于东华大学，2021年我完成了我的学士答辩，2023年结束了我的硕士生涯，同年我拿到了联想的offer，以unity开发工程师的身份，加入联想研究院的大家庭。我在Unity开发领域拥有2年的实际工作经验，目前主要使用Unity引擎进行DW客户端开发。

关于我个人呢，我的爱好比较广泛，我喜欢徒步，本科期间组织过多次徒步活动，和小伙伴一起游山玩水。我喜欢攀岩，研究生期间，代表学校参加了上海市的攀岩锦标赛。我喜欢DIY一些东西，入职联想之后也完成了自己第一个3D打印机。

接下来，我简单介绍我在DWremote团队中的角色与定位

## 角色与定位
我在团队中承担**核心 Unity 开发**的角色，目前我的工作围绕「DW Remote」来开展，从**架构设计、功能开发、性能优化到跨团队协作**，全流程的支撑项目落地。

在架构设计方面，我基于MVC的设计模式，**划分了业务模块**，并定义模块间的交互逻辑，保证项目的可维护性。
在核心功能开发中，我采用了响应式编程的思想，构建了机器人数据管理模块，让客户端对‘用户操作’‘机器人状态变化’的响应更及时。
在性能优化方面，基于**URP 渲染管线**和**GPGPU 技术**，从‘渲染’和‘计算’两个维度对remote进行了优化，降低了控制延迟、压缩了资源体积
最后，remote是多团队协作的结果，所以我需要和算法团队、硬件团队、后端深度联调，推动跨团队问题闭环
接下来，分享一下我主要的工作成果。

## 3DGS渲染部分
第一个成果是我首次将3DGS渲染技术，集成到了DW平台中，让我们的平台具有了虚实融合的关键能力

在项目初期，我首先要解决的是3DGS的工程化落地问题。3DGS SDK的虚实融合方案采用多相机进行渲染，逻辑高度耦合且复杂，导致渲染开销大，性能差。而且复杂的逻辑分散在多个相机和渲染循环中，后续的迭代和问题排查会比较难。
>多相机的虚实融合方案通过多个相机分别渲染虚拟物体和场景地图的深度、纹理信息，通过比较场景地图和虚拟物体的深度，计算纹理的显示优先级

基于对通用渲染管线的深度理解，我实现了一个‘单相机注入式’的虚实融合方案。可以让3DGS无缝地融入到现有的渲染流程中
这里是渲染管线的一个大致流程，我的方案有两个步骤：
1. 第一步，在深度排序之前，将3DGS场景的深度信息注入管线。利用管线自动处理物体的遮挡关系。
2. 第二步，在绘制其他物体之前，我将3DGS的纹理注入管线，作为画面绘制的背景。

新方案只有一个渲染循环，剔除、场景设置等耗时操作只执行一次，之前的方案至少执行三次，**GPU开销比之前降低了5%**，Draw Call也显著减少。在1080P分辨率下可以**节省32MB的显存**。而且这个方案可以轻松地与各种后处理效果结合，拓展性好

我遇到的第二个挑战来自**交互体验**：用户在studio中编辑场景时，会出现严重的画面拖尾现象，破坏沉浸感。

通过深入分析，我定位到根源在于Studio的操作帧与3DGS的渲染帧是异步的。为此，我设计了一个‘帧锚定与双缓冲’的协同机制：

1. Studio的操作指令发出后，必须等待3DGS渲染完成的信号，才更新下一帧的位置，从而在逻辑上对齐了两者的‘时间线’。
2. 同时，我引入了双缓冲机制，确保用户看到的永远是一幅已经完整渲染的图像，彻底消除了画面撕裂和拖尾。实现了平滑的编辑体验。

这套完整的3DGS集成方案，是一个高度模块化、可复用的解决方案。在保证高质量渲染的前提下，GPU耗时减半。它即插即用，对原有场景零侵入，降低了公司内其他项目采纳3DGS技术的门槛。从Studio到现在的Deploy产品，这套方案一直在稳定工作。

## 点云渲染部分
接下来，讲一下我在DW Remote项目中，为移动端构建的一套高性能、低延迟的实时点云渲染管线。提高了我们产品在移动端进行环境感知的能力。

在移动端渲染实时变化的点云，不仅仅是硬件限制的问题，更是资源分配的问题。

我深入调研后发现，我们不能照搬行业内那些‘重型’的、渲染百万级静态点云的方案。因为他们的核心是‘调度海量数据’。而DW Remote的**挑战是‘在极度有限的CPU资源下，保证实时数据的流畅响应’**。”

因为DW Remote首先是一个**高响应的控制应用**，所以CPU资源必须优先保障核心模块。渲染模块如果占用了过多的CPU资源，就会导致应用卡顿，这样就本末倒置了。最初的方案，就有这样的问题。它将大量的解析、转换、分帧逻辑都由CPU来处理，效率低下，而且也不稳定。我将整个渲染流程重新设计，把所有繁重的计算任务，从CPU‘卸载’，迁移到擅长大规模并行计算的GPU上。

在这个新架构下，CPU的职责被简化到极致：仅负责接收网络数据，然后将数据传递给Gpu
GPU会通过成百上千的核心同时对数据进行解析，将点云的位置、颜色信息瞬间解析到显存中。
接下来是一个基于索引的数据管理机制。首次加载走‘全量更新’通道，后续的数据变化则通过‘增量更新’通道，只更新变化的点，极大地降低了数据传输和处理的开销。
为了近一步节省资源，我在GPU侧实现了视锥剔除。从图中可以看到，通过剔除，我们只渲染用户视野内的点，避免了其他不必要的绘制，而且渲染效果也没有受到影响。

这个方案带来的优化是巨大的：
- **首先，它是一个极致的‘CPU友好型’方案。** 将CPU从计算密集型任务中释放，保证了remote的流畅、稳定、高响应。
- 其次，通过GPU并行处理，**点云数据的解析时间缩短为原来的1/10**，实现了从云端数据流到用户眼前的“零延迟”感知。
- 最终，在我们主流的出货设备上，这套方案可以实现**1万点云60FPS的满帧实时渲染**。这意味着用户能够获得实时环境感知体验，这构成了我们产品的一个竞争力。

## 自适应框架

我的第三个成果，是面向机器人构建了一套统一的、自适应的控制框架。解决了过去客户端用户体验不一致、开发效率低下、负载管理困难的三个问题。

我们面临的第一个困境，是**控制生态的碎片化**。我们的客户端需要支持G20专业遥控器和普通安卓手机，但由于缺乏顶层设计，导致控制逻辑与具体硬件深度耦合，形成了多个‘控制孤岛’。这不仅让开发每接入一个新设备，就重写一套控制逻辑，更让用户在切换设备时感到体验割裂

所以我将所有设备的输入信号归一化，向上抽象为统一的‘Action’，通过这个抽象层解耦了硬件设备与控制逻辑，使得所有设备共享一套控制代码。考虑到不同设备的用户体验。我为G20的物理摇杆设计了线性响应曲线，保持了其精准的物理反馈；为手机的虚拟摇杆设计了‘缓入陡出’的非线性曲线，提升手机的微操体验。确保在统一逻辑的基础上，还能提供符合各自设备特性的最优操控手感。

第二个困境，是**机器人功能的碎片化**。不同型号的机器人，其状态和能力千差万别。缺乏统一的管理机制，导致代码中充斥着大量的判断逻辑，随着机器人型号的增加而增加，**整个系统变得脆弱且难以维护**。

在新框架中，我不再关注‘机器人的型号’，而是关注‘机器人的能力’。通过策略模式来对不同型号的机器人进行能力的组装。
这里对不同机器人的数据更新逻辑进行了隔离，机器人具备的能力也进行了定义，这些都会在具体的机器人策略中进行组装。客户端会根据机器人的能力，动态地展示对应的UI。这样一来，过去繁琐的UI适配工作，变成了一个自动化的、可配置的流程，提升了我们适配新机器人的效率。

第三个问题，是机器人负载管理的混乱。不同负载的功能调用逻辑不同，阻碍了负载的模块化扩展。
在这里，我定义了统一的Payload接口，可以通过SDK或外部配置文件来加载负载。使用命令模式，将负载的所有操作封装成独立的、包含自验证和执行逻辑的‘指令’。同时，负载的状态变化会通过统一的事件系统分发出去，驱动UI更新。

框架的整体结构就在这里，这部分是输入模块的展开，这部分是负载部分的展开。这个框架将我们的开发模式从‘面向具体型号’，提升到了‘面向抽象能力’的更高维度。它不仅为用户带来了多端一致的操控体验，更重要的是，它提升了我们团队的开发效率和remote的长期可扩展性。

## Think & Key Learning
好，主要的工作内容就是这些，在这一页我想讲一下我的一些心得和对remote的一些思考。
首先是一些思考，我觉得remote 目前对机器狗的导航模式信息的展示太少了，切换到导航模式之后，用户就失去了对机器狗感知，当狗在导航的时候，用户不知道狗要去哪里？我觉得应该给用户提供更多的信息，比如说是否在导航，目的地坐标是多少，导航的进度是多少。

最后是我的一个体会。技术创造价值,但是技术和产品之间存在巨大的鸿沟，大部分的情况下我们都没办法之间跨越这个鸿沟，只能先走到鸿沟的底部，再慢慢的往上爬，过程很累，但回头看看，风景也不错
## Future
在这一页我想稍稍展望一下未来。
在业务方面，我希望在代码架构这一方面有所提高，架构是一个app的骨骼，数据是血液、逻辑是肌肉，一个好的架构也会让我比较有成就感
在技术方面，我想多了解和学习渲染方面的知识，尤其是GPGPU相关的技术与应用。