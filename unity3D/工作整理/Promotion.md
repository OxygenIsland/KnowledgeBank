---
title: "[[Promotion]]"
type: Permanent
status: ing
Creation Date: 2025-09-30 16:09
tags:
---
## 自我介绍
大家好，我是来自sds的刘博，我毕业于东华大学，本科和研究生均就读于东华大学，2021年我完成了我的学士答辩，2023年结束了我的硕士生涯，同年我拿到了咱们联想的offer，以unity开发工程师的身份，正式加入联想研究院的大家庭。我在Unity开发领域拥有一年的实际工作经验，目前主要使用Unity引擎进行DW客户端开发。

关于我个人呢，我的爱好比较广泛，我喜欢徒步，本科期间组织过多次徒步活动，和小伙伴一起游山玩水。我喜欢攀岩，研究生期间，代表学校参加了上海市的攀岩锦标赛。我喜欢DIY一些东西，入职联想之后也完成了自己第一个3D打印机。

接下来，我简单介绍我在DW前端团队中的角色与定位

## 角色与定位
在DW的架构中，用户可以通过DW Tools 定制化的开发一些内容，并且使用 Editor进行场景构建与编辑，最终发布到DW View中进行呈现。作为一名Unity开发工程师，我主要负责DW View的混合渲染组件和多终端部署。

关于混合渲染，DW的核心理念是一切资产，所以，DW 运营的一个重要职责就是把提升数字孪生混合渲染效果的技术封装成资产组件，方便方案团队快速调用。我的职责之一就是构建这些资产组件。在这部分工作中我首次将3DGS渲染引擎集成到DW平台中，使得用户可以在nerf环境下进行三维场景的编辑；同时我推动了信息展示、位置跳转、自动导览等组件的研发与落地，提升了DW的展览展示效果。

关于多终端部署，多终端部署的愿景就是DW构建的场景资产包可以在不同的终端上展现，并实现无缝切换。在这部分工作中我对ViewerAR的跨平台逻辑进行了整合，并基于此完成了ViewerAR WebGL平台的适配，助力了VIewerAR微信小程序的落地，同时我还参与设计了 DW studio 运营端多平台发布的程序框架，对studio 移动端的输入事件进行了适配

接下来，对我的主要的工作成果，展开分享一下

## 3DGS渲染部分
第一个主要的贡献是我首次将3DGS渲染集成到DW平台中，在这个过程中我遇到了以下2个问题，第一，3DGS多相机的虚实融合方案通过多个相机分别渲染虚拟物体和场景地图的深度、纹理信息，通过比较场景地图和虚拟物体的深度，计算纹理的显示优先级，这个渲染流程是比较复杂的，导致渲染性能没有达到最优；

我结合通用渲染管线，提出了一个单相机的虚实融合方案，其实计算机渲染的过程和画家画画的过程一模一样，先画背景，再画主体。通用渲染管线就是画家，首先他会渲染物体的阴影，下一步他会比较物体的深度信息，这一步决定了物体的绘制顺序，他会先绘制深度信息比较大的物体；接下来才会绘制具体内容，绘制不透明物体、天空盒、半透明物体、UI、最后是后处理。

我根据渲染管线的绘制顺序，在渲染管线比较深度信息之前，将场景地图的深度信息注入到渲染管线中，这样一来，场景地图的深度信息就可以通过管线自动与其他物体进行深度的比较；接着在管线绘制不透明物体之前，我会将3DGS的渲染纹理注入到管线中，这个纹理将会作为帧画面绘制的背景，深度比较小的物体会在这个基础上进行绘制，深度比较大的物体则不会被绘制出来，这样一来就有了一个正确的虚实遮挡关系。

这个单相机的方案只经历**一次**相机渲染循环。剔除和场景设置等比较耗时的CPU操作**只做了一遍**。而优化前的方案做了**至少三次**。这样的优化会极大地降低CPU的开销，同时也会减少Draw Call的数量。
显存方面，新方案的所有操作都在主相机的帧缓冲区内完成。不需要额外渲染纹理。1080分辨率下大约可以节省32MB的显存。
在拓展性方面，虚实融合的关节步骤是在渲染管线的后处理之前的，所以，相比与之前，可以通过添加后处理，来拓展不同的显示效果。

搞定虚实融合的问题之后，我将3DGS的渲染画面整合到了DW中。在实际的编辑操作中，又遇到了第二个问题，拖尾现象破坏了场景编辑的沉浸感，用户体验极差。

从图中可以看到studio旋转移动时，虚拟物体地球和桌子的相对位置不固定，这是由于3DGS的渲染帧和用户的操作帧不同步导致的，3DGS的画面是有延迟的，我做了一些帧同步措施，如图所示，在初始帧的时候对齐3DGS和Studio的初始位姿，第一帧的时候，studio输入了用户的操作信息，nerfModule收到信息后发生给SDK，等待3DGS的画面更新完成之后，再对studio的位置信息进行跟新，保证了一致的渲染效果。

同时针对3DGS的渲染结果，我添加了双缓冲机制，不会直接将正在渲染的图像显示出来。创建一个额外的渲染目标（Render Texture）。渲染线程始终在“后台缓冲区”中绘制，而屏幕上显示的是已经完成渲染的“前台缓冲区”。当一帧渲染完成后，交换这两个缓冲区。这种机制可以确保用户看到的永远是一幅完整的、渲染完毕的图像，从而消除画面撕裂和部分拖尾现象。

从总的GPU耗时来看，这个单相机的方案在渲染质量不变的情况下，GPU耗时减少了一半，而且所有逻辑都集中在单个相机的渲染流程中，更易于维护和迭代，从studio到deploy，3DGS的虚实融合的方案一直在稳定工作，没有经过大的修改。
这个方案是一个**模块化、可复用、符合现代渲染管线思想**的优雅功能。它易于应用，一键集成，即插即用，对原有场景零侵入。这使得我们的sdk更容易被其他项目和开发者所采纳和使用。

## 点云渲染部分

下面来讲一下DWRemote中点云的渲染方案。移动设备因为硬件上的一些限制，他的计算能力本来就有些紧张，那么面对点云这样比较庞大的数据，任何没有经过优化的数据访问都有可能造成线程阻塞，导致GPU空转。而且我们要渲染的点云数据不是一个静态的，它是要跟随环境变化而实时变化的，所以我们的点云数据是要从云端进行加载解析的，这也带来了一些额外的计算压力。

这是我们初始的点云渲染方案，可以看到，这个方案的主要计算都集中到了CPU上，浪费了GPU的计算能力，内存开销巨大，虽然做了分帧优化，但是本质上没有解决问题。

那么目前行业内也有不少移动端点云渲染的解决方案，但是他们的目标是流畅渲染百万千万级的点云，所以他们的数据结构和渲染管线都太重了，这些方案的核心是‘如何用有限的资源去调度无限的数据’。而我们的问题是‘**如何用最少的资源，去渲染有限但实时变化的数据**’，因为DWRemote**不是一个专门的点云渲染器**，CPU除了要处理渲染，它还有更重要的任务：网络通信、解析机器人状态、响应用户UI操作、执行控制逻辑等等。渲染模块绝不能‘喧宾夺主’，把CPU资源吃干抹净，导致整个App失去响应。

我的思路很简单：彻底解放CPU。CPU忙啊，那渲染的事你就别管了。我把点云的解析、变换、剔除这些脏活累活，全部交给真正擅长并行计算的专家——GPU去做。

CPU只做一件事：从网络接收到原始的、压缩的二进制点云数据。然后，它不做任何解析，直接把数据扔给GPU。GPU拿到数据之后会做三件事，分别是数据解析、增量更新、视锥体剔除。
首先大量的GPU核心会同时工作将点云数据解析为一个个点的位置和颜色信息。
接下来我们要将点云的数据保存下来，这些数据是增量更新的基础，如果是第一次收到数据，那我们通过全量更新的管道来保存点云，如果已经不是第一次收到数据了，那我们就通过增量更新的管道来处理发生变化的那部分点云。如何对点云进行管理呢？客户端会和后端约定好数据结构，点云中的每个点都会有一个索引，从而去更新发生变化的点。
ok，无论我们前面通过哪个管道将点云保存下来以后，我们要对点云进行进一步的处理，我们要对GPU的性能进行进一步的节省，程序员都比较小气，不该花的钱，一分都花不了；不该花的性能，1kb也花不了。那么我们还能从哪里再扣一点呢？其实我们收到的点云数据往往是周围环境的一个整体数据，而用户不是每时每刻都想观察这个整体的数据，用户想看的是他感兴趣的那部分，用户会调节相机的视野来观察感兴趣的那部分，通过我们的视锥体剔除的计算，我们会将视野外的点都剔除掉，只渲染用户感兴趣的点云数据。
最后，我们通过间接渲染方式顺便把CPU的工作也减轻了，视锥体剔除计算完之后，GPU能做的事情就做完了，下一步的行动就要请示领导了。请示谁呢？请示CPU，CPU得知GPU完成工作后就开始计算总共有多少个点需要被渲染出来，计算完成之后，告诉GPU你需要渲染300个点，然后GPU去执行渲染指令。但是，cpu的这个计算工作也可以省掉，因为所有的计算都是GPU完成的，GPU对于需要渲染的点是心里有数的，所以CPU不需要计算，只需要说两个字，开始渲染，哦，是4个字，剩下的都交给GPU。


这个渲染方案将CPU从繁重的计算中解放出来，使其能专注于核心的应用逻辑，它是一套**CPU友好型渲染方案**，CPU占用率极低，保障了机器人客户端核心功能的流畅响应。通过将解析工作完全转移至GPU并行处理，**点云的数据解析时间减少为原来的1/10**，实现了数据流的瞬时处理。最终，这套方案在我们配套机器人出货的G20设备上，也能轻松实现**10万点云的60FPS满帧、实时渲染**，为用户带来了极致流畅的视觉体验。

## 自适应框架

目前dwremote支持专业的遥控器G20，以及普通的安卓手机，由于**缺乏统一的输入与控制抽象**，控制代码与具体的硬件设备**深度耦合**。每当需要接入一种新的设备，开发人员就不得不重写大量的适配代码，这无形中形成了一个个封闭的“控制孤岛”。这种碎片化的生态系统，最终导致了跨设备间操控体验的严重割裂，难以形成统一、流畅的用户体验。

remote需要支持不同型号的机器人，这些机器人拥有不同的状态集和能力。在现有的架构下，我们缺乏一套统一且可扩展的状态、功能管理机制。这导致**管理逻辑随着机器人型号的增多而急剧膨胀**。代码中充斥着针对特定型号的判断分支，不仅造成了逻辑上的高度冗余，不同状态之间的更新逻辑也可能相互冲突，为系统的稳定性和后续维护带来了巨大的压力。

对于机器人的核心能力——负载（Payload）的管理，目前也处于一种混乱的状态。由于**缺乏统一的负载接口与管理策略**，不同负载（如云台相机、360全景相机、喊话器）的功能调用的逻辑变得难以维护。这种混乱不仅严重阻碍了机器人功能的模块化扩展，使其难以像“插拔U盘”一样灵活增删功能。

