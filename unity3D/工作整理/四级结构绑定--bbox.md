## 语义分割算法调研
主要有两条思路
1. 根据用户的点击所提供的信息来进行实例分割
2. 比较准确的位姿的算法，每一个实例都具有bbox
   
### 根据用户的点击所提供的信息来进行实例分割
对于一个 mesh 整体或者点云，如下图，用户的点击提供的信息是十分有限的，通过射线检测只能获取到实例的一个面片或者点，根据这个实例中的面片和点去推测实例的 bbox 是十分困难的。但是如果我们提前对 mesh 或者点云进行了语义分割（分割的结果可能不会那么准确），那么用户的点击就可以获取到一个分割的实例，从而计算一个 bounding box，对于分割结果不准确的实例进行 bbox 的手工调整。
![[Pasted image 20241219142824.png|500]]

接下来就是使用 SAM 3D 对场景进行实例分割
#### demo
SAM 3D 利用 SAM 强大的分割能力，不需要对场景进行预训练 zero-shot，零样本部署，但是对于电力场景中不常见的物体的识别可能还需要微调一下
跑了一下 SAM 3D 的 pipelin，结果如下：
1. Step 3 分割最细的一个步骤
![[Pasted image 20241218150056.png|500]]
结果不是很准确，但是个实例大致的位置是可以的
2. 反向融合，合并同类项
针对变电箱中的刀闸，能否顺利识别出来呢？（该模型 step 4 反向融合时，可能会将 SAM 中识别到的细节给覆盖掉，需要调参数）
![[Pasted image 20241217141713.png|500]]
3. 生成一个具有语义结构的场景点云
==未开始==
4. 使用自己的数据进行验证
==未开始==
#### Think
现场的激光扫描效果，表记等微小物体的细节基本丢失（使用点云分割的时候大概率不会分割出单独的实例）
![[Pasted image 20241218145421.png|575]] ![[Pasted image 20241218145334.png|500]]
### 准确的位姿预测
如果有了准确的的位姿预测，每一个实例就具有了 bbox，对于 bbox 结果不准确的物体用户也可以进行手动调整。在 metaspace 地图的训练 pipeline 中，目前使用的是 omniseg 3 dgs 来预测 bounding box，现在存在的问题是，语义层中的 box 信息不全，有的物体没有分割出 box，因为目前的默认的检测结果和语义信息是由基于 coco 训练的 yolo 检测器来的，如果 coco 数据集中没有该类别的物体，yolo 就检测不到，omniseg 3 dgs 就不会生成 3 d bbox，这个问题可以通过重新训练来解决，将新类别的物体添加到数据集中进行训练。

还有一个问题是物体的3D bbox 以及正方向，预测物体的正方向也就是预测物体的 6 D 位姿，在这个领域，目前排名第一的算法是 Foundation pose
   
#### Foundation Pose 的 demo 效果：
1. **需要 CAD 模型**
![[Pasted image 20241218115411.png|500]]
2. **纯图片预测**（先用模型训练出一个 nerf 模型，再用模型进行预测）
![[屏幕截图 2024-12-19 160036 1.png|500]]  ![[Pasted image 20241219155556.png|500]]
通过神经场，能够从不同的视角生成物体的 RGB 图像和深度图，从而为后续的**渲染对比**（render-and-compare）提供数据支持。
3. **自己的数据集验证**
==未开始==
#### FoundationPose 的局限性
![[Pasted image 20241218174846.png|500]]
- **无纹理（Texture-less）**：物体表面缺乏显著的纹理信息，这使得模型难以通过表面细节来准确估计物体的姿势。在这种情况下，传统的基于视觉的特征方法可能无法提供足够的信息来推断物体的正确方向。
    
- **严重遮挡（Severe Occlusion）**：当物体部分或完全被遮挡时，模型失去对物体部分的视觉感知，导致难以从可见部分准确推断出物体的姿势。严重的遮挡尤其在真实世界场景中非常常见，且很难通过单一视角或有限的视觉信息来解决。
    
- **有限的边缘线索（Limited Edge Cues）**：物体的边缘信息对姿势估计非常重要，尤其是在复杂的物体几何形状和环境中。如果边缘信息不清晰或无法从图像中提取，模型就难以准确判断物体的姿势方向
