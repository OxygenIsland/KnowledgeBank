## 2.3.1. 标量
如果你曾经在餐厅支付餐费，那么应该已经知道一些基本的线性代数，比如在数字间相加或相乘。例如，北京的温度为 $52^{\circ}F$（华氏度，除摄氏度外的另一种温度计量单位)。严格来说，仅包含一个数值被称为*标量*（scalar）。如果要将此华氏度值转换为更常用的摄氏度，则可以计算表达式 $c=\frac{5}{9}(f-32)$，并将 $f$ 赋为 $52$。在此等式中，每一项（$5$、$9$ 和 $32$）都是标量值。符号 $c$ 和 $f$ 称为*变量*（variable），它们表示未知的标量值。

本书采用了数学表示法，其中标量变量由普通小写字母表示（例如，$x$、$y$ 和 $z$）。本书用 $\mathbb{R}$ 表示所有（连续）*实数*  标量的空间，之后将严格定义*空间*（space）是什么，但现在只要记住表达式 $x\in\mathbb{R}$ 是表示 $x$ 是一个实值标量的正式形式。符号 $\in$ 称为“属于”，它表示“是集合中的成员”。例如 $x, y \in \{0,1\}$ 可以用来表明 $x$ 和 $y$ 是值只能为 $0$ 或 $1$ 的数字。
(**标量由只有一个元素的张量表示**)。
下面的代码将实例化两个标量，并执行一些熟悉的算术运算，即加法、乘法、除法和指数。
```python
import torch
x = torch.tensor(3.0)
y = torch.tensor(2.0)
x + y, x * y, x / y, x**y
```
`(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))`

## 2.3.2 向量
**向量可以被视为标量值组成的列表**
这些标量值被称为向量的*元素*（element）或*分量*（component）。当向量表示数据集中的样本时，它们的值具有一定的现实意义。例如，如果我们正在训练一个模型来预测贷款违约风险，可能会将每个申请人与一个向量相关联，其分量与其收入、工作年限、过往违约次数和其他因素相对应。如果我们正在研究医院患者可能面临的心脏病发作风险，可能会用一个向量来表示每个患者，其分量为最近的生命体征、胆固醇水平、每天运动时间等。在数学表示法中，向量通常记为粗体、小写的符号（例如，$\mathbf{x}$、$\mathbf{y}$ 和 $\mathbf{z})$）。
人们通过一维张量表示向量。一般来说，张量可以具有任意长度，取决于机器的内存限制。
```python
x = torch.arange(4)
x
```
`tensor([0, 1, 2, 3])`
我们可以使用下标来引用向量的任一元素，例如可以通过 $x_i$ 来引用第 i 个元素。注意，元素 $x_i$ 是一个标量，所以我们在引用它时不会加粗。大量文献认为列向量是向量的默认方向，在本书中也是如此。在数学中，向量 x 可以写为：
$$\mathbf{x} =\begin{bmatrix}x_{1}  \\x_{2}  \\ \vdots  \\x_{n}\end{bmatrix},$$
其中 $x_1,\ldots,x_n$ 是向量的元素。在代码中，我们(**通过张量的索引来访问任一元素**)。
```python
x[3]
```
`4`
### 2.3.2.1. 长度、维度和形状
向量只是一个数字数组，就像每个数组都有一个长度一样，每个向量也是如此。在数学表示法中，如果我们想说一个向量 x 由 n 个实值标量组成，可以将其表示为 x∈Rn。向量的长度通常称为向量的 *维度*（dimension）。
与普通的Python数组一样，我们可以通过调用Python的内置`len()`函数来访问张量的长度。
当用张量表示一个向量（只有一个轴）时，我们也可以通过 `.shape` 属性访问向量的长度。形状（shape）是一个元素组，列出了张量沿每个轴的长度（维数）。对于只有一个轴的张量，形状只有一个元素。
```python
x.shape
```
`torch.Size([4])`
请注意，_维度_（dimension）这个词在不同上下文时往往会有不同的含义，这经常会使人感到困惑。为了清楚起见，我们在此明确一下： _向量_ 或 _轴_ 的维度被用来表示 _向量_ 或 _轴_ 的长度，即向量或轴的元素数量。然而，张量的维度用来表示张量具有的轴数。在这个意义上，张量的某个轴的维数就是这个轴的长度。
## 2.3.3. 矩阵
正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶。矩阵，我们通常用粗体、大写字母来表示 （例如，$\mathbf{X}$、$\mathbf{Y}$ 和 $\mathbf{Z}$），在代码中表示为具有两个轴的张量。
数学表示法使用 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 来表示矩阵 $\mathbf{A}$，其由 $m$ 行和 $n$ 列的实值标量组成。我们可以将任意矩阵$\mathbf{A} \in \mathbb{R}^{m \times n}$视为一个表格，其中每个元素$a_{ij}$属于第$i$行第$j$列：
$$\mathbf{A}=\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}.$$
对于任意$\mathbf{A} \in \mathbb{R}^{m \times n}$，$\mathbf{A}$的形状是（$m$,$n$）或$m \times n$。当矩阵具有相同数量的行和列时，其形状将变为正方形；因此，它被称为*方阵*（square matrix）。当调用函数来实例化张量时，我们可以**通过指定两个分量 $m$ 和 $n$ 来创建一个形状为 $m \times n$ 的矩阵**。
```python
A = torch.arange(20).reshape(5, 4)
A
```

```shell
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15],
        [16, 17, 18, 19]])
```
我们可以通过行索引（$i$）和列索引（$j$）来访问矩阵中的标量元素 $a_{ij}$，例如 $[\mathbf{A}]_{ij}$。如果没有给出矩阵 $\mathbf{A}$ 的标量元素，如在 [[#2.3.2 向量|2.3.2]] 那样，我们可以简单地使用矩阵 $\mathbf{A}$ 的小写字母索引下标 $a_{ij}$ 来引用 $[\mathbf{A}]_{ij}$。为了表示起来简单，只有在必要时才会将逗号插入到单独的索引中，例如 $a_{2,3j}$ 和 $[\mathbf{A}]_{2i-1,3}$。
当我们交换矩阵的行和列时，结果称为矩阵的*转置*（transpose）。通常用 $\mathbf{a}^\top$ 来表示矩阵的转置，如果 $\mathbf{B}=\mathbf{A}^\top$，则对于任意 $i$ 和 $j$，都有 $b_{ij}=a_{ji}$。
因此，矩阵 A的转置是一个形状为 $n \times m$ 的矩阵： 
$$
\mathbf{A}^\top =
\begin{bmatrix}
    a_{11} & a_{21} & \dots  & a_{m1} \\
    a_{12} & a_{22} & \dots  & a_{m2} \\
    \vdots & \vdots & \ddots  & \vdots \\
    a_{1n} & a_{2n} & \dots  & a_{mn}
\end{bmatrix}.
$$
现在在代码中访问(**矩阵的转置**)。
```python
A.T
```

```shell
tensor([[ 0,  4,  8, 12, 16],
        [ 1,  5,  9, 13, 17],
        [ 2,  6, 10, 14, 18],
        [ 3,  7, 11, 15, 19]])
```
作为方阵的一种特殊类型，_对称矩阵_（symmetric matrix）A 等于其转置：A=A⊤。这里定义一个对称矩阵 B：
```python
B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
B
```

```shell
tensor([[1, 2, 3],
        [2, 0, 4],
        [3, 4, 5]])
```
现在我们将 `B` 与它的转置进行比较。
```python
B == B.T
```

```shell
tensor([[True, True, True],
        [True, True, True],
        [True, True, True]])
```

矩阵是有用的数据结构：它们允许我们组织具有不同模式的数据。例如，我们矩阵中的行可能对应于不同的房屋（数据样本），而列可能对应于不同的属性。因此，尽管单个向量的默认方向是列向量，但在表示表格数据集的矩阵中，将每个数据样本作为矩阵中的行向量更为常见。后面的章节将讲到这点，这种约定将支持常见的深度学习实践。例如，沿着张量的最外轴，我们可以访问或遍历小批量的数据样本。
问题，一个张量的维度和一个向量的维度是不是同一种东西?? 请看 [[#向量与张量“维度”的理解|这里]]
## 2.3.4. 张量
就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构。张量（本小节中的“张量”指代数对象）是描述具有任意数量轴的 n 维数组的通用方法。例如，向量是一阶张量，矩阵是二阶张量。张量用特殊字体的大写字母表示（例如，$\mathsf{X}$、$\mathsf{Y}$ 和 $\mathsf{Z}$），它们的索引机制（例如 $x_{ijk}$ 和 $[\mathsf{X}]_{1,2i-1,3}$）与矩阵类似。当我们开始处理图像时，张量将变得更加重要，图像以 $n$ 维数组形式出现，其中3个轴对应于高度、宽度，以及一个*通道*（channel）轴，用于表示颜色通道（红色、绿色和蓝色）。现在先将高阶张量暂放一边，而是专注学习其基础知识。
```python
X = torch.arange(24).reshape(2, 3, 4)
X
```

```shell
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
```
#TODO 其中，`X` 的形状是 `(2, 3, 4)`，它表示 2 个大小为 3x4 的矩阵（或者 4 个大小为 2x3 的矩阵），需要注意的是，这个与一般意义上对矩阵维度的理解有所不同，2、3、4 代表的不是像 x、y、z 这样的维度。但是从一张 rgb 图像的维度上来看，比如一个 shape 为 `(256, 256, 3)` 的张量，可以代表一张分辨率为 256\*256 的彩色图片, 这里又可以理解为 3 个 256x 256 的矩阵。
#### 向量与张量“维度”的理解
**向量的维度**
向量的维度是指该向量所具有的坐标数或分量数。例如，一个[二维向量](https://zhida.zhihu.com/search?content_id=227059400&content_type=Article&match_order=1&q=%E4%BA%8C%E7%BB%B4%E5%90%91%E9%87%8F&zhida_source=entity)拥有两个坐标，通常表示为 (x, y)，其中 x 和 y 是实数。一个三维向量则有三个坐标，通常表示为 (x, y, z)。向量的维度可以是任意正整数，可以表示在 n 维空间中的位置或方向。
**一个向量中含有的数据数量即它的维度。**

**张量的维度**
向量和张量都可以理解为数组，但它们的维度有所不同。向量是一维数组，它由 n 个标量元素组成，可以表示为 n 维列向量或 1 × n 的行向量。向量的维度通常表示为 (n, 1) 或 (n × 1)。张量是多维数组，可以有任意多个维度。二维张量通常表示为矩阵，由 m × n 个标量元素组成，可以表示为 m 行 n 列的矩阵。高维张量可以看作是多个矩阵的集合，每个矩阵的大小和维度相同。张量的维度可以表示为 (n₁, n₂, …, nₖ)，其中 nᵢ 表示张量在第 i 维的大小。可以用一个例子来说明向量和张量的区别。假设有一组三维坐标点，可以用一个三维向量表示一个点，表示为 (x, y, z)。如果有多个点，则可以将它们放在一个二维数组中表示为矩阵，其中每行代表一个点，列代表坐标轴，如下所示： $$ \begin{bmatrix} x₁ & y₁ & z₁ \\ x₂ & y₂ & z₂ \\ ⋮ & ⋮ & ⋮ \\ xₘ & yₘ & zₘ \end{bmatrix} $$ 这就是一个二维张量，它的维度为 (m, 3)。如果要表示多组点，则可以将它们放在一个三维数组中表示为张量，其中每个二维数组代表一个组的点，如下所示： $$ \begin{bmatrix} \begin{bmatrix} x_{1,1} & y_{1,1} & z_{1,1} \\ x_{2,1} & y_{2,1} & z_{2,1} \\ ⋮ & ⋮ & ⋮ \\ x_{m,1} & y_{m,1} & z_{m,1} \end{bmatrix} & \begin{bmatrix} x_{1,2} & y_{1,2} & z_{1,2} \\ x_{2,2} & y_{2,2} & z_{2,2} \\ ⋮ & ⋮ & ⋮ \\ x_{m,2} & y_{m,2} & z_{m,2} \end{bmatrix} & ⋯ & \begin{bmatrix} x_{1,k} & y_{1,k} & z_{1,k} \\ x_{2,k} & y_{2,k} & z_{2,k} \\ ⋮ & ⋮ & ⋮ \\ x_{m,k} & y_{m,k} & z_{m,k} \end{bmatrix} \end{bmatrix} $$ 这就是一个三维张量，它的维度为 (k, m, 3)。

**维度的基础认知**
在我看来，对“维度”二字有两种理解方式，一种是基于物理、空间、几何意义上的理解，比如：“我们生活在三维空间”、“平面是二维的”；一种是数学上的理解，这种理解方式与空间无关，与数据量有关，比如 1∗10 的数组可以看作 10 维向量， 3∗10∗10 的数组可以看作 (3,10,10) 维张量。这两种表述形式在对向量和张量的描述中经常同时出现，容易造成混淆，如果可以在阅读资料时加以区分，可以得到更加准确、直观的理解。常见的表示如下：

基于空间的维度描述 
- “我们生活在三维空间，平面是二维的”。 
- “二维向量 $\vec{x} = (a, b)$ 或 $\boldsymbol{x} = (a, b)$（两种写法为同一描述形式）”：理解为二维空间中的一个线段，即坐标 $(0, 0)$ 到坐标 $(a, b)$ 的有向线段（可以在空间内任意平移）。 
- “三维张量 $\boldsymbol{x} = (a, b, c)$”：理解为一个三维空间，即一个三维立方体，长宽高分别是 $a$, $b$, $c$，切分成小正方体，每个小正方体里有一个数字。 
- “三维向量 $\vec{x} = (a, b, c)$ 或 $\boldsymbol{x} = (a, b, c)$”：理解为三维空间中的线段。 
- “矩阵是二维的”：即矩阵是二维张量，理解为一个平面，平面上被切分成块块，每个块一个数字。 

基于数据量的描述 
- “二维向量 $\vec{x} = (a, b)$ 或 $\boldsymbol{x} = (a, b)$：这个向量中有两个数字。 
- “十维度向量”：同上，向量中有十个数字。 
- “矩阵的维度是 $(3, 10)$” 
- “张量的维度是 $(3, 10, 10)$” 我们惊讶的发现，两种描述对于“向量”的表示是统一的，即由 $n$ 个数组成的向量，可以在几何意义上理解为 $n$ 维空间中的一个线段。 但是张量维度的描述完全不一致，也是造成混淆的关键。

**数组与向量、张量维度的关系**
数组维度的描述也是上述混淆的一大原因。
对于 `int a[3][10][10] = {}`，我们常常会描述为“一个三维数组”，这种描述实际上是将数组看作了一个三维张量（基于空间理解），或者说是一个 `(3, 10, 10)` 维度的张量（基于数据量理解）。
除此之外，上文中我们还提到“`1 * 10` 的数组可以看作 `10` 维向量，`3 * 10 * 10` 的数组可以看作 `(3, 10, 10)` 维张量”，如果只着眼数据的组织，`1 * 10` 的数组更应该表示为 `(1, 10)` 维张量；进一步深入思考，任何 `n` 维向量都可以看作 `(1, n)` 维张量，因此在“以数据量描述维度”的角度，向量、矩阵均可以看作张量的特殊形式，即向量是 `(1, n)` 维张量，矩阵是 `(a, b)` 维张量。

**机器学习领域约定俗成的表示规则**
一般来看，我们更倾向使用基于数据量的表示方法来理解和书写，这种方法在数学书以及机器学习书中常常出现。但是也并非绝对，我们也常常会见到“三维张量”这种描述方式，这种表示方式只能从几何意义的角度上理解。
在我们的书写中，如果目标在于符合数学形式语言的严谨，应当尽可能的描述为“`(3, 10, 10)` 维度的张量”，而不是“三维张量”。
在编程中常常使用譬如的“三维数组”的说法，虽然实用、可以在几何意义上直观理解，但是实际上是欠缺表达能力的，即不能明确表达数据量、数组的大小。
如果对几何意义的理解要求不高，尽可能避免在空间上理解向量、张量，只关注其数据量，是一种很好的避免混淆的方式。

## 2.3.5. 张量算法的基本性质
标量、向量、矩阵和任意数量轴的张量（本小节中的“张量”指代数对象）有一些实用的属性。例如，从按元素操作的定义中可以注意到，任何按元素的一元运算都不会改变其操作数的形状。同样，给定具有相同形状的任意两个张量，任何按元素二元运算的结果都将是相同形状的张量。例如，将两个相同形状的矩阵相加，会在这两个矩阵上执行元素加法。
```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()  # 通过分配新内存，将A的一个副本分配给B
A, A + B
```

```shell
(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [12., 13., 14., 15.],
         [16., 17., 18., 19.]]),
 tensor([[ 0.,  2.,  4.,  6.],
         [ 8., 10., 12., 14.],
         [16., 18., 20., 22.],
         [24., 26., 28., 30.],
         [32., 34., 36., 38.]]))
```
具体而言，**两个矩阵的按元素乘法称为*Hadamard 积*（Hadamard product）（数学符号 $\odot$）**。对于矩阵 $\mathbf{B} \in \mathbb{R}^{m \times n}$，其中第 $i$ 行和第 $j$ 列的元素是 $b_{ij}$。矩阵 $\mathbf{A}$ 和 $\mathbf{B}$ 的 Hadamard 积为： ^47b457
$$
\mathbf{A} \odot \mathbf{B} =
\begin{bmatrix}
    a_{11}  b_{11} & a_{12}  b_{12} & \dots  & a_{1n}  b_{1n} \\
    a_{21}  b_{21} & a_{22}  b_{22} & \dots  & a_{2n}  b_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \dots  & a_{mn}  b_{mn}
\end{bmatrix}.
$$
```python
A * B
```

```shell
tensor([[  0.,   1.,   4.,   9.],
        [ 16.,  25.,  36.,  49.],
        [ 64.,  81., 100., 121.],
        [144., 169., 196., 225.],
        [256., 289., 324., 361.]])
```

将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。
```python
a = 2
X = torch.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape
```

```shell
(tensor([[[ 2,  3,  4,  5],
          [ 6,  7,  8,  9],
          [10, 11, 12, 13]],

         [[14, 15, 16, 17],
          [18, 19, 20, 21],
          [22, 23, 24, 25]]]),
 torch.Size([2, 3, 4]))
```
## 2.3.6. 降维
我们可以对任意张量进行的一个有用的操作是**计算其元素的和**。数学表示法使用 $\sum$ 符号表示求和。为了表示长度为 $d$ 的向量中元素的总和，可以记为 $\sum_{i=1}^dx_i$。在代码中可以调用计算求和的函数：
```python
x = torch.arange(4, dtype=torch.float32)
x, x.sum()
```
`(tensor([0., 1., 2., 3.]), tensor(6.))`
我们可以(**表示任意形状张量的元素和**)。例如，矩阵 $\mathbf{A}$ 中元素的和可以记为 $\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}$。
```python
A.shape, A.sum()
```
`(torch.Size([5, 4]), tensor(190.))`
默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量。 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定`axis=0`。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。
```python
A_sum_axis0 = A.sum(axis=0)
A_sum_axis0, A_sum_axis0.shape
```
`(tensor([40., 45., 50., 55.]), torch.Size([4]))`
指定 `axis=1` 将通过汇总所有列的元素降维（轴1）。因此，输入轴1的维数在输出形状中消失。
```python
A_sum_axis1 = A.sum(axis=1)
A_sum_axis1, A_sum_axis1.shape
```
`(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))`
沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和。
```python
A.sum(axis=[0, 1])  # 结果和A.sum()相同
```
`tensor(190.)`
一个与求和相关的量是 _平均值_（mean 或 average）。我们通过将总和除以元素总数来计算平均值。在代码中，我们可以调用函数来计算任意形状张量的平均值。
```python
A.mean(), A.sum() / A.numel()
```
`(tensor(9.5000), tensor(9.5000))`
同样，计算平均值的函数也可以沿指定轴降低张量的维度。
```python
A.mean(axis=0), A.sum(axis=0) / A.shape[0]
```
`(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))`
### 2.3.6.1. 非降维求和
但是，有时在调用函数来计算总和或均值时保持轴数不变会很有用。
```python
sum_A = A.sum(axis=1, keepdims=True)
sum_A
```

```shell
tensor([[ 6.],
        [22.],
        [38.],
        [54.],
        [70.]])
```
例如，由于 `sum_A` 在对每行进行求和后仍保持两个轴，我们可以通过 [[2.1 数据操作#3 广播机制|广播]] 将 `A` 除以 `sum_A`。
```python
A / sum_A
```

```shell
tensor([[0.0000, 0.1667, 0.3333, 0.5000],
        [0.1818, 0.2273, 0.2727, 0.3182],
        [0.2105, 0.2368, 0.2632, 0.2895],
        [0.2222, 0.2407, 0.2593, 0.2778],
        [0.2286, 0.2429, 0.2571, 0.2714]])
```
如果我们想沿某个轴计算 `A` 元素的累积总和，比如 `axis=0`（按行计算），可以调用 `cumsum` 函数。此函数不会沿任何轴降低输入张量的维度。
```python
A.cumsum(axis=0)
```
**`A.cumsum(axis=0)`**：计算张量 `A` 在第 0 维（即行方向）上的累积和。`axis=0` 表示在列方向上进行累加。
对于 `A.cumsum(axis=0)`，我们会按列对元素进行累加。
- 对于第一列：
```scss
0, 4, 8, 12, 16  -> 0, (0+4)=4, (4+8)=12, (12+12)=24, (24+16)=40
```
最终结果：
```shell
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  6.,  8., 10.],
        [12., 15., 18., 21.],
        [24., 28., 32., 36.],
        [40., 45., 50., 55.]])
```

## 2.3.7. 点积（Dot Product）

我们已经学习了按元素操作、求和及平均值。另一个最基本的操作之一是点积。给定两个向量 $\mathbf{x},\mathbf{y}\in\mathbb{R}^d$，它们的*点积*（dot product）$\mathbf{x}^\top\mathbf{y}$（或 $\langle\mathbf{x},\mathbf{y}\rangle$）是相同位置的按元素乘积的和：$\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i$。
```python
y = torch.ones(4, dtype = torch.float32)
x, y, torch.dot(x, y)
```
`(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))`
注意，我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积：
```python
torch.sum(x * y)
```
`tensor(6.)`
点积在很多场合都很有用。例如，给定一组由向量 $\mathbf{x} \in \mathbb{R}^d$ 表示的值，和一组由 $\mathbf{w} \in \mathbb{R}^d$ 表示的权重。$\mathbf{x}$中的值根据权重$\mathbf{w}$的加权和，可以表示为点积$\mathbf{x}^\top \mathbf{w}$。当权重为非负数且和为1（即$\left(\sum_{i=1}^{d}{w_i}=1\right)$）时，点积表示*加权平均*（weighted average）。将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。本节后面的内容将正式介绍*长度*（length）的概念。
## 2.3.8.矩阵-向量积
现在我们知道如何计算点积，可以开始理解 *矩阵-向量积*（matrix-vector product）。回顾分别在 [[#2.3.3. 矩阵|2.3.3]] 和 [[#2.3.2 向量|2.3.2]] 中定义的矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 和向量 $\mathbf{x} \in \mathbb{R}^n$。让我们将矩阵 $\mathbf{A}$ 用它的行向量表示：
$$\mathbf{A}=
\begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_m \\
\end{bmatrix},$$
其中每个$\mathbf{a}^\top_{i} \in \mathbb{R}^n$都是行向量，表示矩阵的第$i$行。**矩阵向量积$\mathbf{A}\mathbf{x}$是一个长度为$m$的列向量，其第$i$个元素是点积$\mathbf{a}^\top_i \mathbf{x}$**]：
$$
\mathbf{A}\mathbf{x}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_m \\
\end{bmatrix}\mathbf{x}
= \begin{bmatrix}
 \mathbf{a}^\top_{1} \mathbf{x}  \\
 \mathbf{a}^\top_{2} \mathbf{x} \\
\vdots\\
 \mathbf{a}^\top_{m} \mathbf{x}\\
\end{bmatrix}.
$$
我们可以把一个矩阵 $\mathbf{A} \in \mathbb{R}^{m \times n}$ 乘法看作一个从 $\mathbb{R}^{n}$ 到 $\mathbb{R}^{m}$ 向量的转换。这些转换是非常有用的，例如可以用方阵的乘法来表示旋转。后续章节将讲到，我们也可以使用矩阵-向量积来描述在给定前一层的值时，求解神经网络每一层所需的复杂计算。
在代码中使用张量表示矩阵-向量积，我们使用 `mv` 函数。当我们为矩阵 `A` 和向量 `x` 调用 `torch.mv(A, x)` 时，会执行矩阵-向量积。注意，`A` 的列维数（沿轴1的长度）必须与 `x` 的维数（其长度）相同。
```python
A.shape, x.shape, torch.mv(A, x)
```
`(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))`
## 2.3.9. 矩阵-矩阵乘法
在掌握点积和矩阵-向量积的知识后，那么**矩阵-矩阵乘法**（matrix-matrix multiplication）应该很简单。
假设有两个矩阵 $\mathbf{A} \in \mathbb{R}^{n \times k}$ 和 $\mathbf{B} \in \mathbb{R}^{k \times m}$：
$$\mathbf{A}=\begin{bmatrix}
 a_{11} & a_{12} & \cdots & a_{1k} \\
 a_{21} & a_{22} & \cdots & a_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
 a_{n1} & a_{n2} & \cdots & a_{nk} \\
\end{bmatrix},\quad
\mathbf{B}=\begin{bmatrix}
 b_{11} & b_{12} & \cdots & b_{1m} \\
 b_{21} & b_{22} & \cdots & b_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
 b_{k1} & b_{k2} & \cdots & b_{km} \\
\end{bmatrix}.$$
用行向量$\mathbf{a}^\top_{i} \in \mathbb{R}^k$表示矩阵$\mathbf{A}$的第$i$行，并让列向量$\mathbf{b}_{j} \in \mathbb{R}^k$作为矩阵$\mathbf{B}$的第$j$列。要生成矩阵积$\mathbf{C} = \mathbf{A}\mathbf{B}$，最简单的方法是考虑$\mathbf{A}$的行向量和$\mathbf{B}$的列向量:
$$\mathbf{A}=
\begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{bmatrix},
\quad \mathbf{B}=\begin{bmatrix}
 \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
\end{bmatrix}.
$$
当我们简单地将每个元素$c_{ij}$计算为点积$\mathbf{a}^\top_i \mathbf{b}_j$:
$$\mathbf{C} = \mathbf{AB} = \begin{bmatrix}
\mathbf{a}^\top_{1} \\
\mathbf{a}^\top_{2} \\
\vdots \\
\mathbf{a}^\top_n \\
\end{bmatrix}
\begin{bmatrix}
 \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
\end{bmatrix}
= \begin{bmatrix}
\mathbf{a}^\top_{1} \mathbf{b}_1 & \mathbf{a}^\top_{1}\mathbf{b}_2& \cdots & \mathbf{a}^\top_{1} \mathbf{b}_m \\
 \mathbf{a}^\top_{2}\mathbf{b}_1 & \mathbf{a}^\top_{2} \mathbf{b}_2 & \cdots & \mathbf{a}^\top_{2} \mathbf{b}_m \\
 \vdots & \vdots & \ddots &\vdots\\
\mathbf{a}^\top_{n} \mathbf{b}_1 & \mathbf{a}^\top_{n}\mathbf{b}_2& \cdots& \mathbf{a}^\top_{n} \mathbf{b}_m
\end{bmatrix}.
$$
**我们可以将矩阵-矩阵乘法 $\mathbf{AB}$ 看作简单地执行 $m$ 次矩阵-向量积，并将结果拼接在一起，形成一个 $n \times m$ 矩阵**。在下面的代码中，我们在 `A` 和 `B` 上执行矩阵乘法。这里的 `A` 是一个5行4列的矩阵，`B` 是一个4行3列的矩阵。两者相乘后，我们得到了一个5行3列的矩阵。
```python
B = torch.ones(4, 3)
torch.mm(A, B)
```

```shell
tensor([[ 6.,  6.,  6.],
        [22., 22., 22.],
        [38., 38., 38.],
        [54., 54., 54.],
        [70., 70., 70.]])
```
矩阵-矩阵乘法可以简单地称为**矩阵乘法**，不应与 [[#^47b457|Hadamard 积]] 混淆。
## 2.3.10. 范数
线性代数中最有用的一些运算符是 _范数_（norm）。非正式地说，向量的 _范数_ 是表示一个向量有多大。这里考虑的 _大小_（size）概念不涉及维度，而是分量的大小。

在线性代数中，向量范数是将向量映射到标量的函数 f。给定任意向量 x，向量范数要满足一些属性。第一个性质是：如果我们按常数因子α缩放向量的所有元素，其范数也会按相同常数因子的 _绝对值_ 缩放：
$$f(\alpha \mathbf{x}) = |\alpha| f(\mathbf{x}).$$
第二个性质是熟悉的三角不等式:
$$f(\mathbf{x} + \mathbf{y}) \leq f(\mathbf{x}) + f(\mathbf{y}).$$
第三个性质简单地说范数必须是非负的:
$$f(\mathbf{x}) \geq 0.$$
这是有道理的。因为在大多数情况下，任何东西的最小的*大小*是0。最后一个性质要求范数最小为0，当且仅当向量全由0组成。
$$\forall i, [\mathbf{x}]_i = 0 \Leftrightarrow f(\mathbf{x})=0.$$
范数听起来很像距离的度量。欧几里得距离和毕达哥拉斯定理中的非负性概念和三角不等式可能会给出一些启发。事实上，欧几里得距离是一个 $L_2$ 范数：假设 $n$ 维向量 $\mathbf{x}$ 中的元素是 $x_1,\ldots,x_n$，其 **$L_2$*范数* 是向量元素平方和的平方根：**
$$\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2},$$^1a4878
其中，在 $L_2$ 范数中常常省略下标 $2$，也就是说 $\|\mathbf{x}\|$ 等同于 $\|\mathbf{x}\|_2$。在代码中，我们可以按如下方式计算向量的 $L_2$ 范数。
```python
u = torch.tensor([3.0, -4.0])
torch.norm(u)
```
`tensor(5.)`
深度学习中更经常地使用 $L_2$ 范数的平方，也会经常遇到 **$L_1$ 范数，它表示为向量元素的绝对值之和：**
$$\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.$$
与 $L_2$ 范数相比，$L_1$ 范数受异常值的影响较小。为了计算 $L_1$ 范数，我们将绝对值函数和按元素求和组合起来。
```python
torch.abs(u).sum()
```
`tensor(7.)`
$L_2$ 范数和 $L_1$ 范数都是更一般的 $L_p$ 范数的特例：
$$\|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.$$
类似于向量的 $L_2$ 范数，**矩阵** $\mathbf{X} \in \mathbb{R}^{m \times n}$(**的*Frobenius 范数*（Frobenius norm）是矩阵元素平方和的平方根：**
$$\|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}.$$
Frobenius 范数满足向量范数的所有性质，它就像是矩阵形向量的 $L_2$ 范数。调用以下函数将计算矩阵的 Frobenius 范数。
```python
torch.norm(torch.ones((4, 9)))
```
`tensor(6.)`
### 2.3.10.1. 范数和目标
在深度学习中，我们经常试图解决优化问题： _最大化_ 分配给观测数据的概率; _最小化_ 预测和真实观测之间的距离。用向量表示物品（如单词、产品或新闻文章），以便最小化相似项目之间的距离，最大化不同项目之间的距离。目标，或许是深度学习算法最重要的组成部分（除了数据），通常被表达为范数。