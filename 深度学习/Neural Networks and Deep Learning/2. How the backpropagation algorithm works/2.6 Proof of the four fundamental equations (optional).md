We'll now prove the four fundamental equationsÂ (BP1)-(BP4). All four are consequences of the chain rule from multivariable calculus. If you're comfortable with the chain rule, then I strongly encourage you to attempt the derivation yourself before reading on.

Let's begin with Equation (BP1), which gives an expression for the output error, $\delta^L$. To prove this equation, recall that by definition $$ \delta_j^L = \frac{\partial C}{\partial z_j^L}. $$ Applying the chain rule, we can re-express the partial derivative above in terms of partial derivatives with respect to the output activations, $$ \delta_j^L = \sum_k \frac{\partial C}{\partial a_k^L} \frac{\partial a_k^L}{\partial z_j^L}, $$ where the sum is over all neurons $k$ in the output layer. Of course, the output activation $a_k^L$ of the $k^{\text{th}}$ neuron depends only on the weighted input $z_j^L$ for the $j^{\text{th}}$ neuron when $k = j$. And so $\partial a_k^L / \partial z_j^L$ vanishes when $k \neq j$. As a result we can simplify the previous equation to $$ \delta_j^L = \frac{\partial C}{\partial a_j^L} \frac{\partial a_j^L}{\partial z_j^L}. $$ Recalling that $a_j^L = \sigma(z_j^L)$ the second term on the right can be written as $\sigma'(z_j^L)$, and the equation becomes $$ \delta_j^L = \frac{\partial C}{\partial a_j^L} \sigma'(z_j^L), $$ which is just (BP1), in component form.

Next, we'll prove (BP2), which gives an equation for the error $\delta^l$ in terms of the error in the next layer, $\delta^{l+1}$. To do this, we want to rewrite $\delta_j^l = \frac{\partial C}{\partial z_j^l}$ in terms of $\delta_k^{l+1} = \frac{\partial C}{\partial z_k^{l+1}}$. We can do this using the chain rule, $$ \delta_j^l = \frac{\partial C}{\partial z_j^l} $$ $$ = \sum_k \frac{\partial C}{\partial z_k^{l+1}} \frac{\partial z_k^{l+1}}{\partial z_j^l} $$ $$ = \sum_k \frac{\partial z_k^{l+1}}{\partial z_j^l} \delta_k^{l+1}, $$ where in the last line we have interchanged the two terms on the right-hand side, and substituted the definition of $\delta_k^{l+1}$. To evaluate the first term on the last line, note that $$ z_k^{l+1} = \sum_j w_{kj}^{l+1} a_j^l + b_k^{l+1} = \sum_j w_{kj}^{l+1} \sigma(z_j^l) + b_k^{l+1}. $$ Differentiating, we obtain $$ \frac{\partial z_k^{l+1}}{\partial z_j^l} = w_{kj}^{l+1} \sigma'(z_j^l). $$ Substituting back into (42) we obtain $$ \delta_j^l = \sum_k w_{kj}^{l+1} \delta_k^{l+1} \sigma'(z_j^l). $$ This is just (BP2) written in component form.

The final two equations we want to prove are (BP3) and (BP4). These also follow from the chain rule, in a manner similar to the proofs of the two equations above. I leave them to you as an exercise. 

### Exercise 
- Prove Equations (BP3) and (BP4). 
That completes the proof of the four fundamental equations of backpropagation. The proof may seem complicated. But it's really just the outcome of carefully applying the chain rule. A little less succinctly, we can think of backpropagation as a way of computing the gradient of the cost function by systematically applying the chain rule from multi-variable calculus. That's all there really is to backpropagation - the rest is details.