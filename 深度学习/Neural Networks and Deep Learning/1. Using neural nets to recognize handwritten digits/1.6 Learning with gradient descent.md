---
title: "[[1.6 Learning with gradient descent]]"
type: Literature
status: ing
Creation Date: 2025-01-14 09:44
tags:
---
Now that we have a design for our neural network, how can it learn to recognize digits? The first thing we'll need is a data set to learn from - a so-called training data set. We'll use the [MNIST data set](http://yann.lecun.com/exdb/mnist/), which contains tens of thousands of scanned images of handwritten digits, together with their correct classifications. MNIST's name comes from the fact that it is a modified subset of two data sets collected by [NIST](http://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology), the United States' National Institute of Standards and Technology. Here's a few images from MNIST:
![[Pasted image 20250104212329.png|500]]
As you can see, these digits are, in fact, the same as those shown at the beginning of this chapter as a challenge to recognize. Of course, when testing our network we'll ask it to recognize images which aren't in the training set!

The MNIST data comes in (分为) two parts. The first part contains 60,000 images to be used as training data. These images are scanned handwriting samples from 250 people, half of whom were US Census Bureau (人口普查局) employees, and half of whom were high school students. The images are greyscale and 28 by 28 pixels in size. The second part of the MNIST data set is 10,000 images to be used as test data. Again, these are 28 by 28 greyscale images. We'll use the test data to evaluate how well our neural network has learned to recognize digits. To make this a good test of performance, the test data was taken from a _different_ set of 250 people than the original training data (albeit (尽管) still a group split between Census Bureau employees and high school students). This helps give us confidence that our system can recognize digits from people whose writing it didn't see during training.

We'll use the notation $x$ to denote (表示) a training input. It'll be convenient to regard (视为) each training input $x$ as a $28 \times 28 = 784$ -dimensional vector. Each entry (条目) in the vector represents the grey value for a single pixel in the image. We'll denote the corresponding desired (期望) output by $y = y(x)$, where $y$ is a 10-dimensional vector. For example, if a particular training image, $x$, depicts a 6, then $y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T$ is the desired output from the network. Note that $T$ here is the transpose operation, turning a row (行) vector into an ordinary (column 列) vector. 
> [!tip]+ 关于期望输出$y$
> 文中提到的这个期望函数$y(x)$,它的具体的表达式形式，我们是不知道的，它是一种非常复杂的形式，虽然我们不知道它的具体形式，但是我们是知道y(x)的值的，也就是说给定一个$x$，我们是知道$y(x)$具体的值的，所以$y(x)$其实是已知的变量

What we'd like is an algorithm which lets us find weights and biases so that the output from the network approximates (近似) $y(x)$ for all training inputs $x$. To quantify(量化) how well we're achieving this goal we define a cost function (_代价函数_): $$ C(w, b) \equiv \frac{1}{2n} \sum_x \| y(x) - a \|^2. \tag6$$ Here, $w$ denotes the collection of all weights in the network, $b$ all the biases, $n$ is the total number of training inputs, $a$ is the vector of outputs from the network when $x$ is input, and the sum is over all training inputs, $x$. Of course, the output $a$ depends on $x$, $w$ and $b$, but to keep the notation simple I haven't explicitly indicated this dependence. The notation $\| v \|$ just denotes the usual length function(指的是**欧几里得长度（Euclidean length）**，也就是向量的 [[2.3. 线性代数#2.3.10. 范数|L2 范数]]。) for a vector $v$. We'll call $C$ the quadratic (平方) cost function; it's also sometimes known as the mean squared error (均方误差) or just MSE. Inspecting (检查) the form of the quadratic cost function, we see that $C(w, b)$ is non-negative, since every term in the sum is non-negative. Furthermore, the cost $C(w, b)$ becomes small, i.e., $C(w, b) \approx 0$, precisely (精确地) when $y(x)$ is approximately equal to the output, $a$, for all training inputs, $x$. So our training algorithm has done a good job if it can find weights and biases so that $C(w, b) \approx 0$. By contrast (相比之下), it's not doing so well when $C(w, b)$ is large - that would mean that $y(x)$ is not close to the output $a$ for a large number of inputs. ==So the aim of our training algorithm will be to minimize the cost $C(w, b)$ as a function of the weights and biases.== In other words, we want to find a set of weights and biases which make the cost as small as possible. We'll do that using an algorithm known as gradient descent (_梯度下降法_).

Why introduce the quadratic cost? After all, aren't we primarily (主要) interested in the number of images correctly classified by the network? Why not try to maximize that number (指的数字是分类的得分) directly, rather than minimizing a proxy (代理) measure like the quadratic cost? The problem with that is that the number of images correctly classified is not a smooth function of the weights and biases in the network. For the most part, making small changes to the weights and biases won't cause any change at all in the number of training images classified correctly. That makes it difficult to figure out how to change the weights and biases to get improved performance. If we instead use a smooth cost function like the quadratic cost it turns out to be easy to figure out how to make small changes in the weights and biases so as to get an improvement in the cost. That's why we focus first on minimizing the quadratic cost, and only after that（表示时间顺序） will we examine (审查) the classification accuracy.
>- **"only after that will we examine..."**: 这是一个倒装句，正常语序应为 "we will examine the classification accuracy only after that"。倒装结构用于强调时间顺序，即只有在完成第一个动作后，才会进行第二个动作。

Even given that we want to use a smooth cost function, you may still wonder why we choose the quadratic function used in Equation (6). Isn't this a rather (副词，表示程度，相当于“相当”或“颇为”) _ad hoc_(临时的) choice ? Perhaps if we chose a different cost function we'd get a totally different set of minimizing weights and biases? This is a valid concern (问题), and later we'll revisit the cost function, and make some modifications (修改). However, the quadratic cost function of Equation (6) works perfectly well for understanding the basics of learning in neural networks, so we'll stick with it for now.

Recapping (回顾), our goal in training a neural network is to find weights and biases which minimize the quadratic cost function $C(w,b)$. This is a well-posed (明确定义的) problem, but it's got a lot of distracting (分心) structure (结构) as currently posed - the interpretation (解释) of $w$ and b as weights and biases, the [[1.3 Sigmoid neurons#^be6a9a| σ function]] lurking (潜伏) in the background, the choice of network architecture, MNIST, and so on. It turns out (结果) that we can understand a tremendous (巨大的) amount by ignoring most of that structure, and just concentrating on the minimization aspect. So for now we're going to forget all about the specific form of the cost function, the connection to neural networks, and so on. Instead, we're going to imagine that we've simply been given a function of many variables and we want to minimize that function. We're going to develop a technique (技术) called [[2.4 微积分#2.4.3.梯度|gradient ]] descent which can be used to solve such minimization problems. Then we'll come back to the specific function we want to minimize for neural networks. 

Okay, let's suppose we're trying to minimize some function, $C(v)$. This could be any real-valued function of many variables, $v = v_1, v_2, \ldots$. Note that I've replaced the $w$ and $b$ notation by $v$ to emphasize that this could be any function - we're not specifically(特定的) thinking in the neural networks context any more. 
> [!note]+ English knowledge points
> 主语 (Subject): "we" - 指的是说话者或作者以及他们所代表的群体。
> 
> 谓语 (Predicate): "are not specifically thinking" - 这是句子的谓语部分，使用了现在进行时态，表示当前的状态或行为。其中，"are not" 是否定形式，表示否定；"specifically" 是副词，修饰动词 "thinking"，表示思考的方式是特定的或专门的。
> 
> 宾语 (Object): "in the neural networks context" - 这是介词短语，作为 "thinking" 的宾语，表示思考的领域或背景是神经网络。
> 
> 时间状语 (Adverbial of Time): "any more" - 这个短语表示时间的改变，意味着过去可能是在神经网络的背景下思考，但现在不再是这样。


To minimize $C(v)$ it helps to imagine $C$ as a function of just two variables, which we'll call $v_1$ and $v_2$:

![[Pasted image 20250104213652.png|500]]
What we'd like is to find where $C$ achieves(实现) its global minimum(最低限度). Now, of course, for the function plotted(绘制) above, we can eyeball the graph and find the minimum. In that sense, I've perhaps shown slightly _too_ simple a function! A general function, $C$, may be a complicated function of many variables, and it won't usually be possible to just eyeball the graph to find the minimum.

One way of attacking the problem is to use calculus(微积分学) to try to find the minimum analytically(分析地). We could compute derivatives(偏导) and then try using them to find places where $C$ is an extremum(极值). With some luck that might work when $C$ is a function of just one or a few variables. But it'll turn into a nightmare(恶梦) when we have many more variables. And for neural networks we'll often want _far_ more variables - the biggest neural networks have cost functions which depend on billions of weights and biases in an extremely complicated way. Using calculus to minimize that just won't work!

(After asserting(断言) that we'll gain insight(洞察力) by imagining $C$ as a function of just two variables, I've turned around(转身,改变观点) twice in two paragraphs and said, "hey, but what if it's a function of many more than two variables?" Sorry about that. Please believe me when I say that it really does help to imagine $C$ as a function of two variables. It just happens that sometimes that picture breaks down, and the last two paragraphs were dealing with such breakdowns. 
> [!note]+ Picture breaks down
> 描述某种直观的想象、模型或类比在特定情况下不再适用或失效。
> 翻译为  滤镜碎了

Good thinking about mathematics often involves juggling(戏法) multiple intuitive(直观的) pictures, learning when it's appropriate(合适的) to use each picture, and when it's not.)
> [!note]+ 句子结构分析
> 主语 (Subject): "Good thinking about mathematics"
> （关于数学的良好思考）
> 谓语 (Predicate): "involves"
> （涉及）
> 宾语 (Object): 三个并列的动名词短语，描述了良好思考的具体内容：
> "juggling multiple intuitive pictures"
> （灵活运用多种直观图像）
> "learning when it's appropriate to use each picture"
> （学会何时适合使用每种图像）
> "and when it's not"
> （以及何时不适合

Okay, so calculus doesn't work. Fortunately, there is a beautiful analogy(类比) which suggests an algorithm which works pretty well. We start by thinking of our function as a kind of a valley(谷). If you squint(眯眼) just a little at the plot above, that shouldn't be too hard. And we imagine a ball rolling down the slope(坡) of the valley. Our everyday experience tells us that the ball will eventually roll to the bottom of the valley. Perhaps we can use this idea as a way to find a minimum for the function? We'd randomly choose a starting point for an (imaginary) ball, and then simulate the motion of the ball as it rolled down to the bottom of the valley. We could do this simulation simply by computing derivatives(偏导) (and perhaps some second derivatives二阶导数) of $C$ - those derivatives would tell us everything we need to know about the local "shape" of the valley, and therefore(因此) how our ball should roll.

Based on what I've just written, you might suppose that we'll be trying to write down Newton's equations of motion for the ball, considering the effects of friction(摩擦) and gravity, and so on. Actually, we're not going to take the ball-rolling analogy(类比) quite that seriously - we're devising(设计) an algorithm to minimize $C$, not developing an accurate simulation of the laws of physics! The ball's-eye view(小球的视角) is meant to(旨在) stimulate(激发) our imagination, not constrain our thinking. So rather than get into all the messy(凌乱的) details of physics, let's simply ask ourselves: if we were declared(被宣布) God for a day, and could make up(构成) our own laws of physics, dictating(主宰) to the ball how it should roll, what law or laws of motion could we pick that would make it so the ball always rolled to the bottom of the valley?

To make this question(问题) more precise, let's think about what happens when we move the ball a small amount $\Delta v_1$ in the $v_1$ direction, and a small amount $\Delta v_2$ in the $v_2$ direction. Calculus tells us that $C$ changes as follows: $$ \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 + \frac{\partial C}{\partial v_2} \Delta v_2 \tag7 $$ We're going to find a way of choosing $\Delta v_1$ and $\Delta v_2$ so as to make $\Delta C$ negative; i.e., we'll choose them so the ball is rolling down into the valley. 
> [!question]+ 为什么负数才会让小球下降呢？
> 下降也就是让函数的值变小，只要使得函数的偏导数为负数就可以了，换句话说，偏导数为负数的地方就是valley的坡，函数会在这些地方上下降

To figure out how to make such a choice it helps to define $\Delta v$ to be the vector of changes in $v$, $\Delta v \equiv (\Delta v_1, \Delta v_2)^T$, where $T$ is again the transpose operation, turning row vectors into column vectors. We'll also define the [[2.4 微积分#2.4.3.梯度|gradient]] of $C$ to be the vector of partial derivatives, $\left( \frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2} \right)^T$. We denote the gradient vector by $\nabla C$, i.e.: $$ \nabla C \equiv \left( \frac{\partial C}{\partial v_1}, \frac{\partial C}{\partial v_2} \right)^T \tag8$$
In a moment we'll rewrite the change $\Delta C$ in terms of(按照) $\Delta v$ and the gradient, $\nabla C$. Before getting to that, though, I want to clarify something that sometimes gets people hung up(拖延) on the gradient. When meeting the $\nabla C$ notation for the first time, people sometimes wonder how they should think about the $\nabla$ symbol. What, exactly, does $\nabla$ mean? In fact, it's perfectly fine to think of $\nabla C$ as a single mathematical object - the vector defined above - which happens to(恰好) be written using two symbols. In this point of view, $\nabla$ is just a piece of notational flag-waving, telling you "hey, $\nabla C$ is a gradient vector". There are more advanced points of view where $\nabla$ can be viewed as an independent mathematical entity in its own right (for example, as a differential operator), but we won't need such points of view. With these definitions, the expression (7) for $\Delta C$ can be rewritten as $$ \Delta C \approx \nabla C \cdot \Delta v \tag9$$ This equation helps explain why $\nabla C$ is called the gradient vector: $\nabla C$ relates changes in $v$ to changes in $C$, just as we'd expect something called a gradient to do. But what's really exciting about the equation is that it lets us see how to choose $\Delta v$ so as to make $\Delta C$ negative. In particular, suppose we choose $$ \Delta v = -\eta \nabla C, \tag{10}$$ where $\eta$ is a small, positive parameter (known as the learning rate). Then Equation (9) tells us that $\Delta C \approx -\eta \nabla C \cdot \nabla C = -\eta \| \nabla C \|^2$. Because $\| \nabla C \|^2 \geq 0$, this guarantees(担保) that $\Delta C \leq 0$, i.e., $C$ will always decrease, never increase, if we change $v$ according to the prescription(处方) in (10). (Within, of course, the limits of the approximation in Equation (9)). This is exactly the property we wanted! And so we'll take Equation (10) to define the "law of motion" for the ball in our gradient descent algorithm. That is, we'll use Equation (10) to compute a value for $\Delta v$, then move the ball's position $v$ by that amount: $$ v \rightarrow v' = v - \eta \nabla C. \tag{11}$$
> [!info]+ 公式11解释
> $v$：表示当前的参数值（向量）。
> $v′$ 表示更新后的参数值。
> $η$：称为学习率（learning rate），是一个小的正数（如 0.01、0.001）。它控制每次更新的步长；如果$η$ 太小，收敛速度会很慢。如果$η$太大，可能会导致振荡甚至发散。
> $∇C$：表示代价函数 C 的梯度。梯度是一个向量，指向  C 增长最快的方向。在机器学习中，∇C 通常是损失函数对模型参数的偏导数。
> $v−η∇C$：这是梯度下降的更新规则。通过从当前参数值  v 中减去 η∇C，参数会朝着 C 减小的方向移动。
> 
> 直观理解，梯度下降的目标是最小化代价函数 C。每次迭代中，参数v 会沿着 C 的负梯度方向移动一小步（步长由  η 控制）。
> 
> 这个过程就像让一个球从山顶滚下来，每次根据当前坡度（梯度）调整球的位置。

Then we'll use this update rule again, to make another move. If we keep doing this, over and over, we'll keep decreasing $C$ until - we hope - we reach a global minimum. Summing up(总结), the way the gradient descent algorithm works is to repeatedly compute the gradient $\nabla C$, and then to move in the opposite(相反的) direction, "falling down" the slope of the valley. We can visualize it like this:
![[Pasted image 20250104214326.png|500]]
Notice that with this rule gradient descent doesn't reproduce(复制) real physical motion. In real life a ball has momentum, and that momentum may allow it to roll across the slope, or even (momentarily) roll uphill(上坡). It's only after the effects of friction(摩擦力) set in(开始起作用) that the ball is guaranteed to roll down into the valley. By contrast, our rule for choosing $\Delta v$ just says "go down, right now". That's still a pretty good rule for finding the minimum! 

To make gradient descent work correctly, we need to choose the learning rate $\eta$ to be small enough that Equation (9) is a good approximation. If we don't, we might end up with $\Delta C > 0$, which obviously would not be good! At the same time, we don't want $\eta$ to be too small, since that will make the changes $\Delta v$ tiny, and thus the gradient descent algorithm will work very slowly. In practical implementations, $\eta$ is often varied(多种多样的) so that Equation (9) remains a good approximation, but the algorithm isn't too slow. We'll see later how this works.

I've explained gradient descent when $C$ is a function of just two variables. But, in fact, everything works just as well even when $C$ is a function of many more variables. Suppose in particular that $C$ is a function of $m$ variables, $v_1, \ldots, v_m$. Then the change $\Delta C$ in $C$ produced by a small change $\Delta v = (\Delta v_1, \ldots, \Delta v_m)^T$ is $$ \Delta C \approx \nabla C \cdot \Delta v, \tag{12}$$ where the gradient $\nabla C$ is the vector $$ \nabla C \equiv \left( \frac{\partial C}{\partial v_1}, \ldots, \frac{\partial C}{\partial v_m} \right)^T. \tag{13}$$ Just as for the two variable case, we can choose $$ \Delta v = -\eta \nabla C, \tag{14}$$ and we're guaranteed that our (approximate) expression (12) for $\Delta C$ will be negative. This gives us a way of following the gradient to a minimum, even when $C$ is a function of many variables, by repeatedly applying the update rule $$ v \rightarrow v' = v - \eta \nabla C. \tag{15}$$ You can think of this update rule as _defining_ the ==gradient descent algorithm==. It gives us a way of repeatedly changing the position $v$ in order to find a minimum of the function $C$. The rule doesn't always work - several things can go wrong and prevent gradient descent from finding the global minimum of C, a point we'll return to explore in later chapters. But, in practice gradient descent often works extremely well, and in neural networks we'll find that it's a powerful way of minimizing the cost function, and so helping the net learn.

Indeed, there's even a sense in which gradient descent is the optimal(最佳的) strategy(战略) for searching for a minimum. Let's suppose that we're trying to make a move $\Delta v$ in position so as to decrease $C$ as much as possible. This is equivalent to minimizing $\Delta C \approx \nabla C \cdot \Delta v$. We'll constrain(限制) the size of the move so that $\| \Delta v \| = \epsilon$ for some small fixed $\epsilon > 0$. In other words, we want a move that is a small step of a fixed size, and we're trying to find the movement direction which decreases $C$ as much as possible. It can be proved that the choice of $\Delta v$ which minimizes $\nabla C \cdot \Delta v$ is $\Delta v = -\eta \nabla C$, where $\eta = \epsilon / \| \nabla C \|$ is determined by the size constraint $\| \Delta v \| = \epsilon$. So gradient descent can be viewed as a way of taking small steps in the direction which does the most to immediately decrease $C$.
> [!example]+ Exercises
> - Prove the assertion of the last paragraph. _Hint:_ If you're not already familiar with the [Cauchy-Schwarz inequality](http://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality), you may find it helpful to familiarize yourself with it.
> 	如题我们需要证明，在约束$\| \Delta v \| = \epsilon$下，使得$\nabla C \cdot \Delta v$最小的$\Delta v$是$\Delta v = -\eta \nabla C$，其中$\eta = \epsilon / \| \nabla C \|$
> 	 
> - I explained gradient descent when $C$ is a function of two variables, and when it's a function of more than two variables. What happens when $C$ is a function of just one variable? Can you provide a geometric interpretation of what gradient descent is doing in the one-dimensional case?

People have investigated many variations of gradient descent, including variations that more closely mimic(模仿) a real physical ball. These ball-mimicking variations have some advantages, but also have a major disadvantage: it turns out to be necessary to compute second partial derivatives(二阶偏导) of $C$, and this can be quite costly. To see why it's costly, suppose we want to compute all the second partial derivatives $\frac{\partial^2 C}{\partial v_j \partial v_k}$. If there are a million such $v_j$ variables then we'd need to compute something like a trillion (i.e., a million squared) second partial derivatives! That's going to be computationally costly. With that said,(话虽如此) there are tricks for avoiding this kind of problem, and finding alternatives(替代方案) to gradient descent is an active area of investigation(调查). But in this book we'll use gradient descent (and variations) as our main approach to learning in neural networks.

How can we apply gradient descent to learn in a neural network? The idea is to use gradient descent to find the weights $w_k$ and biases $b_l$ which minimize the cost in Equation (6). To see how this works, let's restate(重述) the gradient descent update rule, with the weights and biases replacing the variables $v_j$. In other words, our "position" now has components $w_k$ and $b_l$, and the gradient vector $\nabla C$ has corresponding components $\frac{\partial C}{\partial w_k}$ and $\frac{\partial C}{\partial b_l}$. Writing out the gradient descent update rule in terms of(按照) components, we have $$ w_k \rightarrow w'_k = w_k - \eta \frac{\partial C}{\partial w_k} \tag{16}$$ $$ b_l \rightarrow b'_l = b_l - \eta \frac{\partial C}{\partial b_l}. \tag{17}$$By repeatedly applying this update rule we can "roll down the hill", and hopefully find a minimum of the cost function. In other words, this is a rule which can be used to learn in a neural network. There are a number of challenges in applying the gradient descent rule. We'll look into those in depth in later chapters. But for now I just want to mention one problem. To understand what the problem is, let's look back at the quadratic cost in Equation (6). Notice that this cost function has the form $C = \frac{1}{n} \sum_x C_x$, that is, it's an average(平均的) over costs $C_x = \frac{\|y(x) - a\|^2}{2}$ for individual training examples. In practice, to compute the gradient $\nabla C$ we need to compute the gradients $\nabla C_x$ separately(分别地) for each training input, $x$, and then average them, $\nabla C = \frac{1}{n} \sum_x \nabla C_x$. Unfortunately, when the number of training inputs is very large this can take a long time, and learning thus occurs(发生) slowly.

An idea called stochastic(随机) gradient descent can be used to speed up learning. The idea is to estimate(估计) the gradient $\nabla C$ by computing $\nabla C_x$ for a small sample of randomly chosen training inputs. By averaging over this small sample it turns out that we can quickly get a good estimate of the true gradient $\nabla C$, and this helps speed up gradient descent, and thus learning. To make these ideas more precise, stochastic gradient descent works by randomly picking out a small number $m$ of randomly chosen training inputs. We'll label those random training inputs $X_1, X_2, \ldots, X_m$, and refer to them as a mini-batch. Provided the sample size $m$ is large enough we expect that the average value of the $\nabla C_{X_j}$ will be roughly equal to the average over all $\nabla C_x$, that is, $$ \frac{\sum_{j=1}^m \nabla C_{X_j}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C, \tag{18}$$ where the second sum is over the entire set of training data. Swapping sides we get $$ \nabla C \approx \frac{1}{m} \sum_{j=1}^m \nabla C_{X_j}, \tag{19}$$confirming that we can estimate the overall gradient by computing gradients just for the randomly chosen mini-batch. To connect this explicitly(明确地) to learning in neural networks, suppose $w_k$ and $b_l$ denote the weights and biases in our neural network. Then stochastic gradient descent works by picking out a randomly chosen mini-batch of training inputs, and training with those. $$ w_k \rightarrow w'_k = w_k - \frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{20}$$ $$ b_l \rightarrow b'_l = b_l - \frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial b_l}, \tag{21}$$where the sums are over all the training examples $X_j$ in the current mini-batch. Then we pick out another randomly chosen mini-batch and train with those. And so on, until we've exhausted the training inputs, which is said to complete an epoch of training. At that point we start over with a new training epoch. 
> [!note]+ note
> 也就是说通过许多的mini-batch来训练，直到mini-batch将所有的训练数据都cover，至此完成了一个epoch

Incidentally(顺便), it's worth noting that conventions(约定) vary(各不相同) about scaling(缩放) of the cost function and of mini-batch updates to the weights and biases. In Equation (6) we scaled the overall cost function by a factor $\frac{1}{n}$. People sometimes omit(忽略) the $\frac{1}{n}$, summing over the costs of individual training examples instead of averaging. This is particularly useful when the total number of training examples isn't known in advance. This can occur if more training data is being generated in real time, for instance. And, in a similar way, the mini-batch update rules (20) and (21) sometimes omit the $\frac{1}{m}$ term out the front of the sums. Conceptually(从概念上来说) this makes little difference, since it's equivalent to rescaling the learning rate $\eta$. But when doing detailed comparisons(比较) of different work it's worth watching out for.

We can think of stochastic gradient descent as being like political(政治的) polling(投票): it's much easier to sample a small mini-batch than it is to apply gradient descent to the full batch, just as carrying out a poll is easier than running a full election(选举). For example, if we have a training set of size $n=60,000$, as in MNIST, and choose a mini-batch size of (say) $m=10$, this means we'll get a factor of $6,000$ speedup in estimating the gradient! Of course, the estimate won't be perfect - there will be statistical(统计) fluctuations(波动) - but it doesn't need to be perfect: all we really care about is moving in a general direction that will help decrease $C$, and that means we don't need an exact computation of the gradient. In practice, stochastic gradient descent is a commonly used and powerful technique for learning in neural networks, and it's the basis(基础) for most of the learning techniques we'll develop in this book.
> [!example]+ Exercise
An extreme version of gradient descent is to use a mini-batch size of just 1. That is, given a training input, $x$, we update our weights and biases according to the rules: $$ w_k \rightarrow w'_k = w_k - \eta \frac{\partial C_x}{\partial w_k} $$
$$ b_l \rightarrow b'_l = b_l - \eta \frac{\partial C_x}{\partial b_l} $$
Then we choose another training input, and update the weights and biases again. And so on, repeatedly. This procedure is known as online, on-line, or incremental learning. In online learning, a neural network learns from just one training input at a time (just as human beings do). Name one advantage and one disadvantage of online learning, compared to stochastic gradient descent with a mini-batch size of, say, 20.

Let me conclude(结论) this section by discussing a point that sometimes bugs people new to gradient descent. In neural networks the cost $C$ is, of course, a function of many variables - all the weights and biases - and so in some sense defines a surface in a very high-dimensional space. Some people get hung up thinking: "Hey, I have to be able to visualize all these extra dimensions". And they may start to worry: "I can't think in four dimensions, let alone five (or five million)". Is there some special ability they're missing, some ability that "real" supermathematicians have? Of course, the answer is no. Even most professional mathematicians can't visualize four dimensions especially well, if at all. The trick they use, instead, is to develop other ways of representing what's going on. That's exactly what we did above: we used an algebraic (rather than visual) representation of $\Delta C$ to figure out how to move so as to decrease $C$. People who are good at thinking in high dimensions have a mental(精神的) library containing many different techniques along these lines; our algebraic trick is just one example. Those techniques may not have the simplicity we're accustomed to(习惯) when visualizing three dimensions, but once you build up a library of such techniques, you can get pretty good at thinking in high dimensions. I won't go into more detail here, but if you're interested then you may enjoy reading [this discussion](link_to_discussion) of some of the techniques professional mathematicians use to think in high dimensions. While some of the techniques discussed are quite complex, much of the best content is intuitive and accessible, and could be mastered by anyone.