Having defined neural networks, let's return to handwriting recognition. We can split the problem of recognizing handwritten digits into two sub-problems. First, we'd like a way of breaking an image containing many digits into a sequence of separate images, each containing a single digit. For example, we'd like to break the image
![[Pasted image 20250104211609.png|500]]
Into six separate (分离) images,
![[Pasted image 20250104211621.png|500]]
We humans solve this _segmentation problem_ with ease, but it's challenging for a computer program to correctly break up the image. Once the image has been segmented, the program then needs to classify each individual digit. So, for instance (例如), we'd like our program to recognize that the first digit above,
![[Pasted image 20250104211636.png|31]]
Is a 5.

We'll focus on writing a program to solve the second problem, that is, classifying individual digits. We do this because it turns out that the segmentation problem is not so difficult to solve, once you have a good way of classifying individual digits. There are many approaches to solving the segmentation problem. One approach is to trial (试验) many different ways of segmenting the image, using the individual digit classifier to score each trial segmentation. A trial segmentation gets a high score if the individual digit classifier is confident of its classification in all segments, and a low score if the classifier is having a lot of trouble in one or more segments. The idea is that if the classifier is having trouble somewhere, then it's probably having trouble because the segmentation has been chosen incorrectly. This idea and other variations (变体) can be used to solve the segmentation problem quite well. So instead of worrying about segmentation we'll concentrate on developing a neural network which can solve the more interesting and difficult problem, namely (即), recognizing individual handwritten digits.

To recognize individual digits we will use a three-layer neural network:
![[Pasted image 20250104211707.png|500]]
The input layer of the network contains neurons encoding the values of the input pixels. As discussed (讨论) in the next section, our training data for the network will consist of many 28 by 28 pixel images of scanned handwritten digits, and so the input layer contains 784=28×28 neurons. For simplicity I've omitted (省略) most of the 784 input neurons in the diagram above. The input pixels are greyscale, with a value of 0 representing white, a value of 1.0 representing black, and in between values representing gradually darkening shades (色调) of grey.

The second layer of the network is a hidden layer. We denote (表示) the number of neurons in this hidden layer by n, and we'll experiment with different values for n. The example shown illustrates a small hidden layer, containing just n=15 neurons.

The output layer of the network contains 10 neurons. If the first neuron fires, i.e., has an output ≈1, then that will indicate that the network thinks the digit is a 0. If the second neuron fires then that will indicate that the network thinks the digit is a 1. And so on. A little more precisely, we number the output neurons from 0 through 9, and figure out which neuron has the highest activation (激活) value. If that neuron is, say, neuron number 6, then our network will guess that the input digit was a 6. And so on for the other output neurons.

You might wonder why we use 10 output neurons. After all, the goal of the network is to tell us which digit (0,1,2,…, 9) corresponds (对应) to the input image. A seemingly (似乎) natural way of doing that is to use just 4 output neurons, treating each neuron as taking on a binary value, depending on whether the neuron's output is closer to 0 or to 1. Four neurons are enough to encode the answer, since $2^4=16$ is more than the 10 possible values for the input digit. Why should our network use 10 neurons instead? Isn't that inefficient? The ultimate (最终的) justification (理由) is empirical (经验): we can try out both network designs, and it turns out that, for this particular (特别的) problem, the network with 10 output neurons learns to recognize digits better than the network with 4 output neurons. But that leaves us wondering _why_ using 10 output neurons works better. Is there some heuristic (启发式) that would tell us in advance (提前) that we should use the 10-output encoding instead of the 4-output encoding?

To understand why we do this, it helps to think about what the neural network is doing from first principles (根本原理). Consider first the case where we use 10 output neurons. Let's concentrate (集中) on the first output neuron, the one that's trying to decide whether or not the digit is a 0. It does this by weighing up evidence from the hidden layer of neurons. What are those hidden neurons doing? Well, just suppose for the sake of argument that the first neuron in the hidden layer detects whether or not an image like the following is present:
![[Pasted image 20250104212021.png|247]]
It can do this by heavily weighting input pixels which overlap (重叠) with the image, and only lightly weighting the other inputs. In a similar way, let's suppose for the sake of argument that the second, third, and fourth neurons in the hidden layer detect whether or not the following images are present:
![[Pasted image 20250104212042.png|500]]
As you may have guessed, these four images together make up the 0 image that we saw in the line of digits shown earlier:
![[Pasted image 20250104212106.png|220]]
So if all four of these hidden neurons are firing then we can conclude that the digit is a 0. Of course, that's not the _only_ sort (类别) of evidence we can use to conclude that the image was a 0 - we could legitimately (合法地) get a 0 in many other ways (say (举个例子来说), through translations of the above images, or slight (轻微) distortions (扭曲)). But it seems safe to say that at least in this case we'd conclude that the input was a 0.

Supposing the neural network functions in this way, we can give a plausible (合理的) explanation for why it's better to have 10 outputs from the network, rather than 4. If we had 4 outputs, then the first output neuron would be trying to decide what the most significant (意义) bit of the digit was. And there's no easy way to relate that most significant bit to simple shapes like those shown above. It's hard to imagine that there's any good historical reason the component shapes of the digit will be closely related to (say) the most significant bit in the output.

Now, with all that said, this is all just a heuristic. Nothing says that the three-layer neural network has to operate (操作) in the way I described, with the hidden neurons detecting simple component shapes. Maybe a clever learning algorithm will find some assignment (分配) of weights that lets us use only 4 output neurons. But as a heuristic the way of thinking I've described works pretty well, and can save you a lot of time in designing good neural network architectures (架构).

## Exercise
- There is a way of determining (确定) the bitwise representation of a digit by adding an extra layer to the three-layer network above. The extra layer converts the output from the previous layer into a binary representation, as illustrated in the figure below. Find a set of weights and biases for the new output layer. Assume (假定) that the first 3 layers of neurons are such that the correct output in the third layer (i.e., the old output layer) has activation at least 0.99, and incorrect outputs have activation less than 0.01.
![[Pasted image 20250104212306.png|475]]
### 分析
1. **第三层输出**：第三层有 10 个神经元，分别对应数字 0 到 9。每个神经元的激活值表示对应数字的概率。正确的数字对应的神经元激活值≥0.99，其他神经元的激活值<0.01。
2. **二进制表示**：我们需要将 0 到 9 的数字转换为 4 位二进制数（因为 2^4=16，足够表示 10 个数字）。例如：
    - 0 → 0000
    - 1 → 0001
    - 2 → 0010
    - ...
    - 9 → 1001
3. **新输出层**：新输出层有 4 个神经元，分别对应二进制的 4 位。每个神经元的输出应该是 0 或 1，表示对应二进制位的值。
### 设计新输出层的权重和偏置
我们需要设计一个权重矩阵 W 和偏置向量 b，使得新输出层的输入（第三层的激活值）经过线性变换和激活函数（这里的激活函数我们可以选择阶跃函数，方便输出二进制形式）后，输出正确的二进制表示。
步骤：
1. **定义目标输出**：对于每个数字 0 到 9，定义其对应的 4 位二进制输出。例如：
    - 0 → [0, 0, 0, 0]
    - 1 → [0, 0, 0, 1]
    - 2 → [0, 0, 1, 0]
    - ...
    - 9 → [1, 0, 0, 1]
2. **设计权重矩阵 W**：W 是一个 4×10 的矩阵，每一行对应一个二进制位，每一列对应第三层的一个神经元（数字 0 到 9）。我们可以通过观察二进制表示来设置权重。例如：
    - 第一位（最高位）在数字 8 和 9 时为 1，其他为 0。
    - 第二位在数字 4、5、6、7 时为 1，其他为 0。
    - 第三位在数字 2、3、6、7 时为 1，其他为 0。
    - 第四位在数字 1、3、5、7、9 时为 1，其他为 0。
    因此，权重矩阵 W 可以设计为：
    $$
   W = \begin{bmatrix}
   0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 \\
   0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 \\
   0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 \\
   0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\
   \end{bmatrix}
   $$
3. **设计偏置向量 \( b \)**：偏置向量 \( b \) 是一个 4 维向量，用于调整每个二进制位的输出。我们可以设置偏置为-0.5，这样当输入激活值为 1 时，输出为 0.5，经过激活函数后输出为 1；当输入激活值为 0 时，输出为-0.5，经过激活函数后输出为 0。
   因此，偏置向量 \( b \) 可以设计为：
   $$b = \begin{bmatrix}
   -0.5 \\
   -0.5 \\
   -0.5 \\
   -0.5 \\
   \end{bmatrix}$$
   4. **激活函数**：使用阶跃函数作为激活函数，当输入大于 0 时输出 1，否则输出 0。
### 验证
假设输入数字 2，第三层的激活值为：$a = [0, 0, 0.99, 0, 0, 0, 0, 0, 0, 0]$
经过新输出层的计算：$z = W \cdot a + b$
计算得到：
$$z = \begin{bmatrix}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 0 \\
0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 \\
0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 \\
0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 1 \\
\end{bmatrix}
\cdot
\begin{bmatrix}
0 \\
0 \\
0.99 \\
0 \\
0 \\
0 \\
0 \\
0 \\
0 \\
0 \\
\end{bmatrix}
+
\begin{bmatrix}
-0.5 \\
-0.5 \\
-0.5 \\
-0.5 \\
\end{bmatrix}
=
\begin{bmatrix}
0 \\
0 \\
0.99 \\
0 \\
\end{bmatrix}
+
\begin{bmatrix}
-0.5 \\
-0.5 \\
-0.5 \\
-0.5 \\
\end{bmatrix}
=
\begin{bmatrix}
-0.5 \\
-0.5 \\
0.49 \\
-0.5 \\
\end{bmatrix}
$$
经过激活函数后：
$$\text{output} = \begin{bmatrix}
0 \\
0 \\
1 \\
0 \\
\end{bmatrix}$$
即二进制表示 0010，对应数字 2，验证通过。