---
title: "[[1.3 Sigmoid neurons]]"
type: Literature
status: done
Creation Date: 2025-01-13 09:45
tags:
---
Learning algorithms sound terrific (了不起). But how can we devise such algorithms for a neural network? Suppose we have a network of perceptrons that we'd like to use to learn to solve some problem. For example, the inputs to the network might be the raw pixel data from a scanned, handwritten image of a digit (数字). And we'd like the network to learn weights and biases so that the output from the network correctly classifies the digit. To see how learning might work, suppose we make a small change in some weight (or bias) in the network. What we'd like is for this small change in weight to cause only a small corresponding change in the output from the network. As we'll see in a moment, this property will make learning possible. Schematically (如图), here's what we want (obviously this network is too simple to do handwriting recognition!):
![[Pasted image 20250103161711.png|487]]
If it were true that a small change in a weight (or bias) causes only a small change in output, then we could use this fact to modify the weights and biases to get our network to behave more in the manner (方式) we want. For example, suppose the network was mistakenly classifying an image as an "8" when it should be a "9". We could figure out how to make a small change in the weights and biases so the network gets a little closer to classifying the image as a "9". And then we'd repeat this, changing the weights and biases over and over to produce better and better output. The network would be learning.

The problem is that this isn't what happens when our network contains perceptrons. In fact, a small change in the weights or bias of any single perceptron in the network can sometimes cause the output of that perceptron to completely flip, say from 0 to 1. That flip may then cause the behaviour of the rest of the network to completely change in some very complicated way. So while your "9" might now be classified correctly, the behaviour of the network on all the other images is likely to have completely changed in some hard-to-control way. That makes it difficult to see how to gradually modify the weights and biases so that the network gets closer to the desired behaviour. Perhaps there's some clever way of getting around this problem. But it's not immediately obvious how we can get a network of perceptrons to learn.

We can overcome this problem by introducing a new type of artificial neuron called a _sigmoid_ neuron. Sigmoid neurons are similar to perceptrons, but modified so that ==small changes in their weights and bias cause only a small change in their output==. That's the crucial (至关重要的) fact which will allow a network of sigmoid neurons to learn.

Okay, let me describe the sigmoid neuron. We'll depict (描绘) sigmoid neurons in the same way we depicted perceptrons:
![[Pasted image 20250103161756.png|305]]
Just like a perceptron, the sigmoid neuron has inputs, $x_1, x_2, \ldots$. But instead of being just 0 or 1, these ==inputs== can also take on any values between 0 and 1. So, for instance, 0.638... is a valid input for a sigmoid neuron. Also just like a perceptron, the sigmoid neuron has weights for each input, $w_1, w_2, \ldots$, and an overall bias, $b$. But the output is not 0 or 1. Instead, it's $\sigma(w \cdot x + b)$, where $\sigma$ is called the sigmoid function, and is defined by: $$ \sigma(z) \equiv \frac{1}{1 + e^{-z}} \tag{3}$$ To put it all a little more explicitly (说得更明确一点), the output of a sigmoid neuron with inputs $x_1, x_2, \ldots$, weights $w_1, w_2, \ldots$, and bias $b$ is $$ \frac{1}{1 + \exp(-\sum_j w_j x_j - b)} \tag{4}$$ At first sight, sigmoid neurons appear very different to perceptrons. The algebraic (代数的) form of the sigmoid function may seem opaque and forbidding if you're not already familiar with it. In fact, there are many similarities between perceptrons and sigmoid neurons, and the algebraic form of the sigmoid function turns out to be more of a technical detail than a true barrier (障碍) to understanding. ^be6a9a

To understand the similarity to the perceptron model, suppose $z \equiv w \cdot x + b$ is a large positive number. Then $e^{-z} \approx 0$ and so $\sigma(z) \approx 1$. In other words, when $z = w \cdot x + b$ is large and positive, the output from the sigmoid neuron is approximately (大约) 1, just as it would have been for a perceptron (它好像在支持感知器). Suppose on the other hand that $z = w \cdot x + b$ is very negative. Then $e^{-z} \rightarrow \infty$, and $\sigma(z) \approx 0$. So when $z = w \cdot x + b$ is very negative, the behaviour of a sigmoid neuron also closely approximates a perceptron. It's only when $w \cdot x + b$ is of modest (谦虚的) size that there's much deviation (偏差) from the perceptron model.

What about the algebraic form of σ? How can we understand that? In fact, the exact form of σ isn't so important - what really matters is the shape of the function when plotted. Here's the shape:
![[Pasted image 20250103175608.png|300]]

This shape is a smoothed out version of a step function:
![[Pasted image 20250103175636.png|300]]

If $\sigma$ had in fact been a step function, then the sigmoid neuron would be a perceptron, since the output would be 1 or 0 depending on whether $w \cdot x + b$ was positive or negative*. By using the actual $\sigma$ function we get, as already implied above, a smoothed out perceptron. Indeed, it's the smoothness of the $\sigma$ function that is the crucial fact, not its detailed (详细的) form. ==The smoothness of $\sigma$ means that small changes $\Delta w_j$ in the weights and $\Delta b$ in the bias will produce a small change $\Delta \text{output}$ in the output from the neuron==. In fact, calculus (微积分) tells us that $\Delta \text{output}$ is well approximated by $$ \Delta \text{output} \approx \sum_j \frac{\partial \text{output}}{\partial w_j} \Delta w_j + \frac{\partial \text{output}}{\partial b} \Delta b, \tag{5}$$
where the sum is over all the weights $w_j$ , and $\frac{\partial \text{output}}{\partial w_j}$ and $\frac{\partial \text{output}}{\partial b}$ denote partial derivatives (衍生品) of the output with respect to $w_j$ and $b$, respectively. Don't panic (恐慌) if you're not comfortable with partial derivatives! While the expression above looks complicated, with all the partial derivatives, it's actually saying something very simple (and which is very good news): $\Delta \text{output}$ is a linear function of the changes $\Delta w_j$ and $\Delta b$ in the weights and bias. ==This linearity makes it easy to choose small changes in the weights and biases to achieve any desired small change in the output.== So while sigmoid neurons have much of the same qualitative behaviour as perceptrons, they make it much easier to figure out how changing the weights and biases will change the output. 

If it's the shape of $\sigma$ which really matters, and not its exact form, then why use the particular form used for $\sigma$ in Equation (3)? In fact, later in the book we will occasionally (偶尔地) consider neurons where the output is $f(w \cdot x + b)$ for some other activation function $f(\cdot)$. The main thing that changes when we use a different activation function is that the particular values for the partial derivatives (偏微分) in Equation (5) change. It turns out that when we compute those partial derivatives later, using $\sigma$ will simplify the algebra (代数), simply because exponentials (指数) have lovely properties when differentiated. In any case, $\sigma$ is commonly-used in work on neural nets, and is the activation function we'll use most often in this book.

How should we interpret(解释) the output from a sigmoid neuron? Obviously, one big difference between perceptrons and sigmoid neurons is that sigmoid neurons don't just output 0 or 1. They can have as output any real number between 0 and 1, so values such as 0.173… and 0.689… are legitimate (合法的) outputs. This can be useful, for example, if we want to use the output value to represent the average intensity (密度) of the pixels in an image input to a neural network. But sometimes it can be a nuisance (讨厌的). Suppose we want the output from the network to indicate either "the input image is a 9" or "the input image is not a 9". Obviously, it'd be easiest to do this if the output was a 0 or a 1, as in a perceptron. But in practice (实践中) we can set up a convention (约定) to deal with this, for example, by deciding to interpret any output of at least 0.5 as indicating a "9", and any output less than 0.5 as indicating "not a 9". I'll always explicitly state (申明) when we're using such a convention, so it shouldn't cause any confusion.

## Exercises 
**Sigmoid neurons simulating perceptrons, part I** 
- Suppose we take all the weights and biases in a network of perceptrons, and multiply them by a positive constant, $c > 0$. Show that the behaviour of the network doesn't change.  
**Sigmoid neurons simulating perceptrons, part II** 
- Suppose we have the same setup as the last problem - a network of perceptrons. Suppose also that the overall input to the network of perceptrons has been chosen. We won't need the actual input value, we just need the input to have been fixed. Suppose the weights and biases are such that $w \cdot x + b \neq 0$ for the input $x$ to any particular perceptron in the network. Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant $c > 0$. Show that in the limit as $c \to \infty$ the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons. How can this fail when $w \cdot x + b = 0$ for one of the perceptrons?

### 1. **感知机（Perceptron）与 Sigmoid 神经元的区别**

- **感知机**的输出是二值的，基于 \( w \cdot x + b \) 的符号：$$\text{输出} = \begin{cases}
  1 & \text{如果 } w \cdot x + b > 0, \\
  0 & \text{如果 } w \cdot x + b \leq 0.
  \end{cases}$$
- **Sigmoid 神经元**的输出是连续的，基于 Sigmoid 函数：
  $$\sigma (z) = \frac{1}{1 + e^{-z}},$$
  其中  $z = w \cdot x + b$。Sigmoid 神经元的输出范围是  (0, 1) 。
---
### 2. **权重和偏置缩放 \( c \) 倍**
当权重和偏置被缩放 \( c > 0 \) 倍时，Sigmoid 神经元的输入变为：
$$Z = c (w \cdot x + b).$$
此时，Sigmoid 神经元的输出为：
$$\sigma (z) = \frac{1}{1 + e^{-c (w \cdot x + b)}}.$$  

---

### 3. **当 $c \to \infty$  时**

- 如果 $w \cdot x + b > 0$ ，当  $c \to \infty$  时，$c (w \cdot x + b) \to +\infty$ ，此时：$$\sigma (z) = \frac{1}{1 + e^{-c (w \cdot x + b)}} \to 1.$$
- 如果 $w \cdot x + b < 0$，当 $c \to \infty$ 时，$c (w \cdot x + b) \to -\infty$，此时：
  $$\sigma (z) = \frac{1}{1 + e^{-c (w \cdot x + b)}} \to 0.$$
因此，当 $c \to \infty$ 时，Sigmoid 神经元的输出趋近于感知机的二值输出：
$$\sigma (z) \to \begin{cases}
1 & \text{如果 } w \cdot x + b > 0, \\
0 & \text{如果 } w \cdot x + b < 0.
\end{cases}$$
这表明，当 $c \to \infty$  时，Sigmoid 神经元网络的行为与感知机网络完全相同。

---

### 4. **当 $\cdot x + b = 0$  时的问题**

如果某个感知机的输入满足 $w \cdot x + b = 0$，那么对于 Sigmoid 神经元：
$$\sigma (z) = \frac{1}{1 + e^{-c (w \cdot x + b)}} = \frac{1}{1 + e^{0}} = \frac{1}{2}.$$
无论 \( c \) 取多大，Sigmoid 神经元的输出始终为 \( 0.5 \)，而感知机的输出在这种情况下是未定义的（因为 $w \cdot x + b = 0$是决策边界）。因此，当 $w \cdot x + b = 0$ 时，Sigmoid 神经元网络的行为无法与感知机网络一致。