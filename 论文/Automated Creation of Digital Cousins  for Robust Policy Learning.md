## Abstract:
Digital Twins：需要完全精确地重建现实世界场景的细节，包括每一个微小的元素。它强调对真实场景的**高保真重建**，创建成本高，泛化能力弱
Digital Cousins：重点关注保留更高层次的细节，比如空间关系和语义属性，不需要显式复制真实世界的场景，而是通过**保持相似的几何形状和语义属性**来与真实环境保持一定的相似性。针对 Digital Twins 的两个缺点
- 论文提出了一种**自动化创建 Digital Cousins 的方法，借此可以自动生成用于训练机器人的**完全交互式场景。
- 这种方法支持从真实到仿真再到真实（real-to-sim-to-real）的全自动化管道，意味着可以在仿真环境中训练机器人，并在现实环境中 zero-shot 部署（zero-shot learning 的核心思想是，模型能够通过已经学到的知识，推断出对于未见过的任务或数据的合适处理方式。）
![[Pasted image 20241107173526.png]]
## ACDC ：自动化数字表亲创建系统
![[Pasted image 20241108104153.png]]
### **步骤 1：提取步骤（Extraction Step） 从输入的 RGB 图像中提取每个物体的相关信息**
- **输入数据**：系统首先接收一张由**校准相机**拍摄的**单张RGB图像**，用来生成数字表亲。图像被标记为 **X**，并且相机的**内参矩阵**为 **K**。
- **物体检测与信息提取**：在这一阶段，系统会使用**计算机视觉技术**（如目标检测、分割等方法）来分析图像，识别出其中的各个物体，并提取与每个物体相关的**几何和语义信息**。这些信息可能包括：
    - 物体的形状、大小、位置等几何特征。
    - 物体的语义属性，如物体的类别（例如，椅子、桌子等）以及它们的功能或用途（例如，是否可抓取，是否为障碍物等）。
这个步骤的核心是从输入图像中提取出每个物体的**结构化信息**，为后续的数字表亲匹配提供基础，具体流程如下：
1. **GPT-4生成物体标签**：
	- GPT-4模型会根据图像内容为每个物体生成描述，并且每个物体的标签被标记为 **cj**，其中 **j ∈ {1, ..., M}** 表示生成的物体标签数量（**M** 是物体数量）。
2. **GroundedSAM-v2生成物体掩模**：
	- 接着，生成的标签 **cj** 会与输入的 RGB 图像 **X** 一起输入到**GroundedSAM-v2**模型中。
	- **GroundedSAM-v2**是一个基于**视觉-语言**模型的物体检测工具，可以根据给定的标签和图像信息来检测物体，并生成每个物体的**mask**，即该物体在图像中的位置和形状。
	- 生成的掩模标记为 **mi**，其中 **i ∈ {1, ..., N}**，表示检测到的物体数量。每个物体掩模 **mi** 对应一个在图像中检测到的物体区域。
3. **同步标签和掩模**：
	 - 通过**重新提示 GPT-4**，让它根据生成的物体掩模（**mi**）为每个掩模选择一个准确的标签（**li**）。也就是说，GPT-4需要从之前生成的标签列表中选择一个**合适的标签**来描述每个物体掩模。
4. DepthAnything-v2 深度预估、点云重建
	- **DepthAnything-v2**是一个**单目深度估计模型**，能够从**单张 RGB 图像**中推断出深度图。
	- 使用相机的内参矩阵 **K**（包括焦距、主点等信息）将深度图转换为**点云**（point cloud）。
	- ACDC 就可以生成一组包含**物体标签（li）**、**物体掩模（mi）**、**物体点云（pi）** 和**物体像素（xi）** 的 **物体表示** （**oi**）。具体表示如下：
												${oi=(li,mi,pi,xi)}_{i=1}^{N}$

### 步骤 2：匹配步骤（Matching Step）
- **资产类别**：每个虚拟资产（数字表亲）在数据集中都会被赋予一个**语义类别**（**ti**），表示它属于什么类型的物体（例如：椅子、桌子、橱柜等）。
- **资产快照**：每个资产模型具有多个**不同方向的快照**（**iis**），这些快照展示了资产在不同角度下的样子。每个资产有一个代表性的快照 **Ii**，用于表示该资产的标准外观。所有这些快照组合成一个资产元组，形式为：
									    $ai = (ti, Ii, \{iis\}_{s=1}^{Nsnap})$
    
    其中，**ti**是资产类别，**Ii**是代表性快照，${iis}_{s=1}^{Nsnap}$ 是该资产的多个快照， Nsnap 表示快照的数量。
1.  匹配候选类别
	- 给定一个输入物体表示（**oi**），首先需要通过计算**CLIP 相似度**得分来选取候选的资产类别。**CLIP**（Contrastive Language-Image Pretraining）是一种将图像和文本关联起来的深度学习模型，通过它可以计算输入标签（**li**）与资产数据集中的类别名称（**ti**）之间的**语义相似度**。
	- 选择 $k_{cat}$ 个最接近的类别，这些类别对应的是可能的匹配资产类型。
2. 选择数字表亲候选
	- 选定的**候选类别**中，系统会计算 **DINOv2特征嵌入距离** 来从这些类别中选择潜在的数字表亲候选。**DINOv2**是一种强大的视觉特征提取模型，它通过计算物体图像（例如掩模区域内的 RGB 图像**xi**）与每个资产代表性快照（**Ij**） 之间的特征嵌入距离，来评估物体与数字表亲的相似性。
	- 选择 $k_{cand}$ 个最接近的数字表亲候选。
3. 细化匹配与选择最终数字表亲
	- 一旦选择了初步的候选数字表亲，系统会对每个候选数字表亲的所有**快照**（**iis**）计算**DINOv2嵌入距离**。即对于每个候选资产的不同方向的快照，系统会重新计算与输入物体区域的匹配度。
	- 最后，系统会选择**kcous**个最接近的数字表亲（即最符合要求的虚拟资产），每个选中的数字表亲包括：
	    - **虚拟资产（Ac）**：选择的虚拟资产。
	    - **特定方向（qc）**：基于选中的快照，确定的虚拟资产的方向。==当前视角下的资产的方向，怎么能确定资产在场景下的位置和方向呢?==
### 步骤 3：生成步骤（Generation Step）
1. **物体放置和缩放**
	- **物体的中心对齐**：给定输入物体的信息 **oi** 和对应匹配的数字表亲信息 **(Ac, qc)**，系统首先会将数字表亲的**边界框中心**（bounding box center）对齐到**输入物体的点云质心**（centroid）。点云质心是物体在三维空间中的中心点，通常是通过计算点云中所有点的平均位置来获得。
	- **缩放物体**：接着，数字表亲会根据物体点云的**扩展范围**（extents）进行缩放，以确保数字表亲的大小与原始物体的大小一致。这意味着，物体的大小和比例会根据输入图像中的物体尺寸进行调整，从而确保其在场景中有合适的比例和位置。
2. **地面和墙面拟合**
	- **拟合平面**：系统还会从物体的**点云数据**中提取出**地面**和**墙面**的平面信息。这一步是通过拟合**地面平面**和**墙面平面**来实现的，目的是确定场景中物体放置的物理环境。==怎么提取??==
    - 例如，通过点云中的数据，系统可以识别哪些物体应该放置在地面上，哪些物体应该放置在墙面上。
	- **GPT-4询问**：在拟合地面和墙面平面后，系统会使用**GPT-4**模型来判断是否有任何物体应该挂在墙上或放置在地面上。GPT-4通过理解场景的语义，可以帮助确定哪些物体的放置方式是合适的（例如，是否应该将某些物体挂在墙上而不是放在地面上）。
3. **物体去重叠**
	- **去重叠处理**：为了确保仿真场景的物理稳定性，系统会进行**去重叠处理**（de-penetration）。系统会确保场景中的物体不会相互穿透或重叠。
	    - 物体的碰撞检测会被用于此目的，任何两个物体如果相互重叠，系统会调整它们的位置，使它们不再穿透或重叠，确保物理稳定性。
4. **场景后处理**
	- **附加的场景后处理**：在上述步骤完成后，系统会进行额外的**场景后处理**，例如微调物体的定位、调整物体的表面属性、改进物理交互等，确保场景的真实性和互动性。


